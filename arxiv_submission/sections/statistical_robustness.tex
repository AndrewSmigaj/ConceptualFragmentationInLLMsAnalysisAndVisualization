\section{Statistical Robustness of Cluster Structures}

\subsection{Cross-Layer Path Analysis Metrics}

To rigorously analyze the semantic meaning and evolution of concepts across layers, we introduce the following cross-layer metrics that quantify relationships between clusters at different depths of the network.

\subsubsection{Centroid Similarity ($\rho^c$)}

Centroid similarity measures whether clusters with the same or different IDs across layers represent similar concepts in the embedding space:

\begin{equation}
\rho^c(C_i^l, C_j^{l'}) = \text{sim}(\mu_i^l, \mu_j^{l'})
\end{equation}

where $\mu_i^l$ is the centroid of cluster $i$ in layer $l$, and sim is a similarity function (cosine similarity or normalized Euclidean distance). For meaningful comparison across layers with different dimensionalities, we project clusters into a shared embedding space using techniques such as PCA or network-specific dimensionality reduction.

High centroid similarity between clusters in non-adjacent layers (e.g., $C_0^{l_1}$ and $C_0^{l_3}$) indicates conceptual ``return'' or persistence, even when intermediate layers show different cluster assignments. This helps identify stable semantic concepts that temporarily fragment but later reconverge.

\subsubsection{Membership Overlap ($J$)}

Membership overlap quantifies how data points from a cluster in one layer are distributed across clusters in another layer:

\begin{equation}
J(C_i^l, C_j^{l'}) = \frac{|D_i^l \cap D_j^{l'}|}{|D_i^l \cup D_j^{l'}|}
\end{equation}

where $D_i^l$ is the set of data points in cluster $i$ at layer $l$. This Jaccard similarity metric reveals how cohesive groups of data points remain across network depth. A related metric is the membership containment ratio:

\begin{equation}
\text{contain}(C_i^l \rightarrow C_j^{l'}) = \frac{|D_i^l \cap D_j^{l'}|}{|D_i^l|}
\end{equation}

Which measures what proportion of points from an earlier cluster appear in a later cluster. High containment suggests the second cluster has ``absorbed'' the concept represented by the first.

\subsubsection{Trajectory Fragmentation Score ($F$)}

Trajectory fragmentation quantifies how consistently a model processes semantically similar inputs:

\begin{equation}
F(p) = H(\{t_{i,j} | (i,j) \in p\})
\end{equation}

where $p$ is a path, $t_{i,j}$ is the transition from cluster $i$ to cluster $j$, and $H$ is an entropy or variance measure. Low fragmentation scores indicate stable, predictable paths through the network layers. We also compute fragmentation relative to class labels:

\begin{equation}
F_c(y) = \frac{|\{p | \exists x_i, x_j \in X_y \text{ where } \text{path}(x_i) \neq \text{path}(x_j)\}|}{|X_y|}
\end{equation}

This measures the proportion of samples with the same class label $y$ that follow different paths, directly quantifying concept fragmentation within a semantic category.

\subsubsection{Inter-Cluster Path Density (ICPD)}

ICPD analyzes higher-order patterns in concept flow by examining multi-step transitions:

\begin{equation}
\text{ICPD}(C_i^l \rightarrow C_j^{l'} \rightarrow C_k^{l''}) = \frac{|\{x | \text{cluster}^l(x) = i \wedge \text{cluster}^{l'}(x) = j \wedge \text{cluster}^{l''}(x) = k\}|}{|\{x | \text{cluster}^l(x) = i\}|}
\end{equation}

This metric identifies common patterns like:
\begin{itemize}
    \item Return paths ($i \rightarrow j \rightarrow i$): The model temporarily assigns inputs to an intermediate concept before returning them to the original concept
    \item Similar-destination paths ($i \rightarrow j \rightarrow k$ where $\rho^c(C_i^l, C_k^{l''})$ is high): The model reaches a conceptually similar endpoint through an intermediate step
\end{itemize}

These patterns reveal how the network refines its representations across layers. We visualize the most frequent paths using weighted directed graphs, where the weight of each edge represents the transition frequency.

\subsubsection{Application to Interpretability}

These cross-layer metrics provide a quantitative foundation for analyzing how concepts evolve through a neural network. By combining them with the ETS clustering approach, we can generate interpretable explanations of model behavior, such as:

``Inputs in this category initially cluster together (low $F_c$), then separate based on feature X (high membership divergence at layer 2), before reconverging in the output layer (high centroid similarity between first and last layer clusters).''

This statistical foundation ensures that our interpretations are not merely post-hoc narratives but are grounded in measurable properties of the network's internal representations.

\subsection{Path Reproducibility Across Seeds}

To assess structural stability, we define dominant archetypal paths as frequent cluster sequences across datapoints. We compute Jaccard overlap and recurrence frequency of these paths across different random seeds or bootstrapped model runs. High path recurrence suggests the presence of model-internal decision logic rather than sampling artifacts. 

For clusters $(L_lC_k)$ and $(L_mC_j)$, let $S_k^l = \{i \mid \text{datapoint } i \text{ in } L_lC_k\}$. We compute Jaccard similarity, $J(S_k^l, S_j^m) = \frac{|S_k^l \cap S_j^m|}{|S_k^l \cup S_j^m|}$, to measure datapoint retention across layers. High overlap between clusters -- with high centroid similarity suggests stable group trajectories. 

We also compute the frequency of similarity-convergent paths by aggregating transitions where the final cluster resembles an earlier one, e.g., $[L_1C_k \rightarrow L_2C_j \rightarrow L_3C_m]$ where $\text{cos}(\mathbf{c}_k^1, \mathbf{c}_m^3) > 0.9$. Density is calculated as $D = \sum_{\text{similarity-convergent paths}} T^1_{kj} T^2_{jm} \cdot \mathbb{1}[\text{cos}(\mathbf{c}_k^1, \mathbf{c}_m^3) > \theta]$, where $T^l_{kj}$ is the transition count from $(L_lC_k)$ to $L_{(l+1)}C_j$. High density suggests latent funnels where datapoints converge to similar activation spaces.

\subsection{Trajectory Coherence}

For a datapoint $(i)$ with path $\pi_i = [c_i^1, c_i^2, \dots, c_i^L]$, we compute the fragmentation score using subspace angles between consecutive centroid transitions: 
\begin{align}
F_i = \frac{1}{L-2} \sum_{t=2}^{L-1} \arccos\left(\frac{(\mathbf{c}_{c_i^{t+1}}^{t+1} - \mathbf{c}_{c_i^t}^t) \cdot (\mathbf{c}_{c_i^t}^t - \mathbf{c}_{c_i^{t-1}}^{t-1})}{\|\mathbf{c}_{c_i^{t+1}}^{t+1} - \mathbf{c}_{c_i^t}^t\| \|\mathbf{c}_{c_i^t}^t - \mathbf{c}_{c_i^{t-1}}^{t-1}\|}\right)
\end{align}

Low $F_i$ indicates coherent trajectories, especially in similarity-convergent paths.

\subsection{Feature Attribution for Cluster Transitions}

To understand why datapoints transition between clusters, we apply feature attribution methods such as Integrated Gradients (IG) or SHAP. These methods identify which input features (e.g., specific tokens in text) most influence the activation changes driving cluster transitions. For instance, if a datapoint moves from L1C0 to L2C2, IG can reveal that the token ``excellent'' was pivotal in this shift. This attribution is computed by integrating gradients along the path from a baseline input to the actual input, highlighting feature importance for each transition.

\subsection{Path Interestingness Score}

We define an ``interestingness'' score for paths to identify those that are particularly noteworthy. The score combines:
\begin{itemize}
    \item Transition Rarity: The inverse frequency of each transition, highlighting uncommon jumps.
    \item Similarity Convergence: The cosine similarity between starting and ending clusters.
    \item Coherence: The inverse of the fragmentation score. 
\end{itemize}

The interestingness score is computed as $I(\pi_i) = \alpha \cdot \text{rarity}(\pi_i) + \beta \cdot \text{sim}(\mathbf{c}_{c_i^1}^1, \mathbf{c}_{c_i^L}^L) + \gamma \cdot \frac{1}{F_i}$, where $\alpha, \beta, \gamma$ are tunable weights. Paths with high interestingness are prioritized for LLM narration.