\section{Conclusion}

Concept Trajectory Analysis (CTA) provides a method for understanding how neural networks organize and process information through their layers. Our analysis of GPT-2 with 1,228 words across 8 semantic categories reveals it organizes language primarily by grammatical function, with 48.5\% (95\% CI: 45.7\%–51.2\%) of words converging to what we term an ``entity superhighway.'' The highly significant grammatical clustering ($\chi^2 = 95.90$, $p < 0.0001$) demonstrates that neural language models develop organizational principles fundamentally different from semantic categorization.

We developed this finding using our framework that combines mathematical analysis with LLM-generated interpretations. Using CTA within our Concept MRI visualization tool, we tracked how 1,228 words across 8 semantic categories move through GPT-2's layers, observing a reorganization from semantic differentiation (26 paths) to grammatical consolidation (5 paths). The stability analysis revealed phase transitions where semantic clustering gives way to syntactic organization.

We also applied CTA to medical AI applications. Our heart disease diagnosis study shows how neural networks process patient data through distinct pathways. The analysis identified four primary pathways corresponding to different patient profiles: Archetype 1 tends to process younger, lower-risk patients; Archetype 4 typically handles elderly patients with elevated cholesterol, showing 71% disease prevalence in our dataset. Path fragmentation scores appear to correlate with diagnostic uncertainty—patients with ambiguous clinical indicators often show higher fragmentation. This analysis could help understand model behavior in medical contexts.

Our integration of cross-layer metrics—centroid similarity ($\rho^c$), membership overlap ($J$), and trajectory fragmentation ($F$)—provides a mathematically grounded framework for quantifying concept evolution. The use of Gap statistic for optimal cluster determination ensures statistically valid groupings, while LLM-powered analysis translates complex patterns into domain-meaningful narratives. This combination addresses the longstanding challenge of making neural network internals both rigorously analyzable and humanly understandable.

Our findings, while preliminary, suggest interesting directions for future research. If confirmed by larger studies, the observation that GPT-2 may prioritize grammatical over semantic organization could inform our understanding of neural network information processing. The relatively low computational overhead of CTA makes it practical for analyzing larger models, though more work is needed to validate its effectiveness across different architectures and domains.

As neural networks grow in complexity and impact, interpretability methods become increasingly important. Our work suggests that neural networks may organize information differently from human intuition. By making these organizational principles more visible, CTA contributes to the broader effort of developing interpretable AI systems.

\subsection{Limitations and Failure Modes}

While CTA provides valuable insights into neural network organization, several limitations and failure modes warrant discussion:

\subsubsection{Technical Limitations}

\begin{itemize}
    \item \textbf{Clustering instability}: When activation spaces lack clear structure, clustering results may vary significantly across runs. Low silhouette scores or high variance in cluster assignments indicate unreliable trajectories.
    \item \textbf{Scalability challenges}: Very deep networks (100+ layers) pose computational and interpretability challenges. Tracking trajectories through many layers can obscure rather than clarify patterns.
    \item \textbf{High-dimensional curse}: In extremely high-dimensional activation spaces, distance metrics become less meaningful, potentially leading to arbitrary cluster assignments.
\end{itemize}

\subsubsection{Interpretation Risks}

\begin{itemize}
    \item \textbf{Spurious patterns}: Random fluctuations might appear as meaningful paths, especially with small sample sizes. Statistical validation (as we demonstrated with $\chi^2$ tests) is essential.
    \item \textbf{LLM hallucination}: Generated narratives may sound plausible while misrepresenting actual patterns. Cross-validation with quantitative metrics is crucial.
    \item \textbf{Correlation vs. causation}: CTA reveals organizational patterns but cannot establish causal relationships. Interventional studies are needed to verify causal claims.
\end{itemize}

\subsubsection{Application Boundaries}

\begin{itemize}
    \item \textbf{Architecture dependence}: CTA works best with feedforward architectures. Recurrent or highly branched architectures may require adaptation.
    \item \textbf{Domain transfer}: Patterns discovered in one domain (e.g., language) may not transfer to others (e.g., vision) without careful validation.
    \item \textbf{Training dynamics}: CTA analyzes trained models. Understanding how these patterns emerge during training requires additional analysis.
\end{itemize}

Despite these limitations, CTA's mathematical grounding, statistical validation, and empirical successes (discovering grammatical organization in GPT-2, identifying clinical pathways in medical AI) demonstrate its value as an interpretability tool. Responsible use involves acknowledging these limitations, validating findings through multiple approaches, and maintaining appropriate skepticism about generated narratives.
