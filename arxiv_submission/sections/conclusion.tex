\section{Conclusion}

Concept Trajectory Analysis (CTA) with LLM-powered interpretation represents a significant advancement in neural network interpretability, combining mathematical rigor with human-understandable explanations. Our work establishes a foundation for analyzing how concepts evolve and transform as they propagate through neural networks, providing insights into both the computational and semantic aspects of model behavior.

The integration of cross-layer metrics such as centroid similarity, membership overlap, and fragmentation scores provides a robust framework for quantifying concept evolution, while LLM-generated narratives translate these patterns into domain-meaningful explanations. Our experiments on traditional datasets (Titanic, Heart Disease) and comprehensive GPT-2 analysis demonstrate that this approach can identify nuanced decision processes, semantic organization patterns, and potential biases that might otherwise remain opaque.

The GPT-2 case studies reveal how transformer models systematically organize semantic knowledge, progressing from syntactic to semantic representations across layers. Our semantic pivot analysis shows how models handle contradictory information, while the semantic subtypes study demonstrates layer-specific clustering strategies for different conceptual categories. The per-layer ETS threshold optimization reveals that optimal clustering criteria vary systematically across network depth, providing new insights into transformer interpretability.

By formalizing the conditions under which activation-space clustering is valid and establishing methods to assess path stability, we have addressed critical gaps in the theoretical foundation of cluster-based interpretability. The incorporation of Explainable Threshold Similarity (ETS) further enhances transparency by providing verbalizably transparent membership conditions that can be directly communicated to domain experts.

This work represents a step toward bridging the divide between mathematical precision and human understanding in interpretability research, offering tools that can help researchers, developers, and end-users better understand the internal workings of neural networks. As models continue to grow in complexity and impact, approaches like CTA that combine quantitative analysis with qualitative explanation will become increasingly important for ensuring transparency, fairness, and trustworthiness in AI systems.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/cluster_entropy.png}
    \caption{Cluster entropy analysis showing the distribution of information across the network's internal representation spaces.}
    \label{fig:cluster_entropy}
\end{figure}