\section{LLM-Powered Analysis for Cluster Paths}

Recent advances in large language models (LLMs) provide new opportunities for interpreting neural network behavior through the analysis of cluster paths. We introduce a systematic framework for leveraging LLMs to generate human-readable narratives and insights about the internal decision processes represented by cluster paths.

\subsection{LLM Integration Architecture}

Our framework integrates LLMs into the cluster path analysis pipeline through a modular architecture with three primary components:

\begin{enumerate}
    \item \textbf{Cluster Labeling}: LLMs analyze cluster centroids to generate meaningful semantic labels that describe the concepts each cluster might represent.
    \item \textbf{Path Narrative Generation}: LLMs create coherent narratives explaining how concepts evolve through the network as data points traverse different clusters.
    \item \textbf{Bias Audit}: LLMs analyze demographic statistics associated with paths to identify potential biases in model behavior.
\end{enumerate}

The architecture includes:

\begin{itemize}
    \item \textbf{Cache Management}: Responses are cached to enable efficient re-analysis and promote reproducibility
    \item \textbf{Prompt Optimization}: Specialized prompting techniques that improve consistency and relevance of generated content
    \item \textbf{Batch Processing}: Efficient parallel processing of multiple clusters and paths
    \item \textbf{Demography Integration}: Analysis of how cluster paths relate to demographic attributes
\end{itemize}

\subsection{Semantic Cluster Labels}

The cluster labeling process transforms abstract mathematical representations (centroids) into semantically meaningful concepts. Our experiments with the Titanic and Heart Disease datasets demonstrated that LLMs can generate consistent, meaningful labels even for similar clusters, distinguishing subtle differences in their representations.

% Include generated cluster labels
\input{sections/generated/titanic_labels}
\input{sections/generated/heart_labels}

\subsection{Path Narratives}

The narrative generation process explains how concepts evolve as data traverses the network. These narratives provide several interpretability advantages:

\begin{enumerate}
    \item \textbf{Contextual Integration}: Incorporating cluster labels, convergent points, fragmentation scores, and demographic data creates multi-faceted narratives.
    \item \textbf{Conceptual Evolution}: Narratives explain how concepts transform and evolve through network layers.
    \item \textbf{Decision Process Insights}: Explanations reveal potential decision-making processes that might be occurring within the model.
    \item \textbf{Demographic Awareness}: Including demographic information ensures narratives consider fairness and bias implications.
\end{enumerate}

% Include generated path narratives (These are now superseded by the _report.tex files)
% \input{sections/generated/titanic_narratives} 
% \input{sections/generated/heart_narratives}

% \subsection{Bias Audit Results} % Entire subsection commented out
% 
% The bias audit component analyzes potential demographic biases in cluster paths, creating a comprehensive analysis that:
% 
% \begin{enumerate}
%     \item \textbf{Identifies Demographic Patterns}: Reveals which demographic factors most strongly influence clustering patterns.
%     \item \textbf{Quantifies Bias}: Uses statistical measures (Jensen-Shannon divergence) to quantify deviation from baseline demographic distributions.
%     \item \textbf{Highlights Problematic Paths}: Identifies specific paths with high bias scores for further investigation.
%     \item \textbf{Provides Mitigation Strategies}: Offers concrete recommendations for addressing identified biases.
% \end{enumerate}
% 
% % Include generated bias metrics
% \input{sections/generated/titanic_bias}
% \input{sections/generated/heart_bias}

% Comprehensive GPT-4 reports
\subsection{LLM Consolidated Reports}
\input{sections/generated/titanic_report} % This should contain the cleaned Titanic narratives
\input{sections/generated/heart_report}   % This should contain the cleaned Heart narratives

\subsection{Fragmentation Metrics Overview}

To ground the LLM narratives in quantitative evidence, we compute 
three complementary fragmentation measures for every layer and for each
archetypal path:

\begin{description}
    \item[Path--Centroid Fragmentation (\textit{FC})]  Measures how
    dissimilar consecutive clusters are along a specific sample path.
    It is defined as $\mathrm{FC}=1-\overline{\mathrm{sim}}$, where
    $\overline{\mathrm{sim}}$ is the mean centroid similarity
    (cosine) between successive clusters on the path.  High values
    indicate that the representation for a data point ``jumps'' across
    concept regions between layers; low values indicate a coherent,
    incremental refinement.

    \item[Intra--Class Cluster Entropy (\textit{CE})]  For every
    layer we cluster activations and measure the Shannon entropy of
    the resulting cluster distribution \emph{within} each ground--truth
    class.  The entropy is normalised by $\log_2 k^*$ (the selected
    number of clusters) so that $\textit{CE}=1$ means class features
    are maximally dispersed, whereas $\textit{CE}=0$ means that each
    class occupies a single, compact cluster.

    \item[Sub--space Angle Fragmentation (\textit{SA})]  We compute
    the principal components for the activations of each class and
    evaluate the pair-wise principal angles between those subspaces.
    Large mean angles ($\gg 0^\circ$) imply that the network embeds
    classes in orthogonal directions—evidence of fragmentation—while
    small angles suggest a shared, low–dimensional manifold.
\end{description}

The optimal cluster count $k^*$ chosen by the silhouette criterion is
reported alongside the metrics.  Empirically we find:
\begin{itemize}
    \item Layers with \textit{CE} $\uparrow$ and \textit{SA} $\uparrow$
          correlate with high \textit{FC} in their outgoing paths,
          indicating simultaneous dispersion across samples and
          conceptual jumps along individual trajectories.
    \item Decreases in $k^*$ often coincide with lower entropy and
          angle, signalling that the network is condensing disparate
          features into fewer, more stable concepts as depth
          increases.
\end{itemize}

All three metrics are provided to the LLM as part of the prompt so
that narrative explanations can tie qualitative descriptions to
quantitative evidence (e.g., ``\emph{entropy drops sharply from layer~2
 to layer~3, indicating that the network consolidates passenger class
 information}'').

\begin{figure}[t]
    \centering
    \includegraphics[width=0.32\textwidth]{figures/optimal_clusters.png}
    \includegraphics[width=0.32\textwidth]{figures/cluster_entropy.png}
    \includegraphics[width=0.32\textwidth]{figures/subspace_angle.png}
    \caption{Layer-wise quantitative fragmentation metrics.
    Left: optimal $k^*$ per layer;
    middle: normalised intra-class cluster entropy (CE);
    right: mean sub-space angle (SA).}
    \label{fig:fragmentation_metrics}
\end{figure}

\begin{table}[h!]
\centering
\caption{Layer-wise fragmentation metrics for the Titanic dataset model.}
\label{tab:fragmentation_metrics_titanic}
\begin{tabular}{lcccc}
\toprule
Layer & $k^*$ & CE & SA ($^\circ$) & FC (path mean) \\
\midrule
Layer 1 & 10 & 0.818 & 39.6 & 0.432 \\
Layer 2 &  2 & 0.784 & 33.5 & 0.432 \\
Layer 3 &  2 & 0.779 & 25.7 & 0.432 \\
Output  &  2 & 0.761 & 12.9 & 0.432 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h!]
\centering
\caption{Layer-wise fragmentation metrics for the Heart dataset model.}
\label{tab:fragmentation_metrics_heart}
\begin{tabular}{lcccc}
\toprule
Layer & $k^*$ & CE & SA ($^\circ$) & FC (path mean) \\
\midrule
Layer 1 & 2 & 0.722 & 16.3 & 0.096 \\
Layer 2 & 2 & 0.713 & 11.5 & 0.096 \\
Layer 3 & 2 & 0.711 &  7.8 & 0.096 \\
Output  & 2 & 0.702 &  3.1 & 0.096 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Advantages and Limitations}

\textbf{Advantages}:
\begin{enumerate}
    \item \textbf{Interpretable Insights}: Converts complex mathematical patterns into human-readable explanations.
    \item \textbf{Multi-level Analysis}: Provides insights at cluster, path, and system-wide levels.
    \item \textbf{Bias Detection}: Proactively identifies potential fairness concerns in model behavior.
    \item \textbf{Integration with Metrics}: Combines qualitative narratives with quantitative fragmentation and similarity metrics.
\end{enumerate}

\textbf{Limitations}:
\begin{enumerate}
    \item \textbf{Potential for Overinterpretation}: LLMs might ascribe meaning to patterns that are artifacts of the clustering process.
    \item \textbf{Domain Knowledge Gaps}: Analysis quality depends on the LLM's understanding of the specific domain.
    \item \textbf{Computational Cost}: Generating narratives for many paths can be resource-intensive.
    \item \textbf{Validation Challenges}: Verifying the accuracy of generated narratives requires domain expertise.
\end{enumerate}

% The following placeholder prose was part of an earlier draft and is now
% superseded by automatically generated cluster labels, narratives and bias
% tables inserted via \input.  To avoid contradictory text we comment it out.
\iffalse
Our experiments show that these narratives can effectively translate complex mathematical relationships into intuitive explanations that capture the essence of the model's internal behavior.

### 6.4 Bias Auditing Through LLMs
... (placeholder content removed) ...
\fi