\section{GPT-2 Case Study: Grammatical Organization in Neural Language Models}

We analyze how GPT-2 organizes linguistic information by tracking 1,228 single-token words across 8 semantic categories through the model's layers. Our analysis indicates that GPT-2 primarily organizes words by grammatical function rather than semantic meaning, with evidence of convergence from initial semantic differentiation to grammatical organization in later layers.

\subsection{Experimental Design}

We designed a systematic experiment to study how GPT-2 organizes semantic knowledge using 1,228 validated single-token words across 8 semantically distinct categories with balanced grammatical representation:

\begin{itemize}
    \item \textbf{Concrete Nouns}: Physical objects (e.g., ``table'', ``mountain'', ``book'')
    \item \textbf{Abstract Nouns}: Conceptual entities (e.g., ``freedom'', ``justice'', ``emotion'')
    \item \textbf{Physical Adjectives}: Observable properties (e.g., ``tall'', ``smooth'', ``bright'')
    \item \textbf{Emotive Adjectives}: Emotional descriptors (e.g., ``joyful'', ``melancholy'', ``serene'')
    \item \textbf{Manner Adverbs}: How actions are performed (e.g., ``quickly'', ``carefully'', ``boldly'')
    \item \textbf{Degree Adverbs}: Intensity modifiers (e.g., ``extremely'', ``barely'', ``quite'')
    \item \textbf{Action Verbs}: Dynamic processes (e.g., ``run'', ``create'', ``destroy'')
    \item \textbf{Stative Verbs}: State descriptions (e.g., ``exist'', ``belong'', ``resemble'')
\end{itemize}

\subsubsection{Dataset Construction}

We curated 1,228 validated single-token words distributed across semantic subtypes through systematic linguistic analysis, achieving balanced grammatical representation: 275 nouns (22.4%), 280 adjectives (22.8%), 267 adverbs (21.7%), and 406 verbs (33.1%). Each word was verified for single-token representation in GPT-2's tokenizer and semantic category membership, ensuring clean experimental conditions for concept trajectory analysis.

\subsubsection{Novel Methodological Innovations}

Our analysis introduced several key innovations:

\begin{itemize}
    \item \textbf{Unified CTA with Gap Statistic}: Optimal k determination using Gap statistic (k=4 for layer 0, k=2 for layers 1-11)
    \item \textbf{Windowed Analysis}: Temporal segmentation for phase transition detection (see Section 3.5)
    \item \textbf{Unique Cluster Labeling}: Every cluster assigned unique ID (e.g., L4\_C1) to prevent cross-layer confusion
    \item \textbf{Trajectory Visualization}: Sankey diagrams showing concept flow through network
    \item \textbf{Comprehensive Path Analysis}: Tracking ALL paths (not just archetypal), revealing 26→8→5 path convergence
\end{itemize}

\subsection{Key Findings}

\subsubsection{Grammatical Organization Patterns}

Our analysis suggests that GPT-2 organizes words primarily by grammatical function rather than semantic meaning:

\begin{itemize}
    \item \textbf{Layer 0}: 4 clusters showing semantic differentiation (animals, objects, properties, abstracts)
    \item \textbf{Layers 1-11}: Rapid consolidation to just 2 clusters (entities vs. modifiers)
    \item \textbf{Convergence rate}: 48.5\% (95\% CI: 45.7\%–51.2\%) of all words converge to the most common pathway
    \item \textbf{Path reduction}: 26 unique paths → 8 paths → 5 paths across windows
\end{itemize}

\begin{table}[h!]
\centering
\caption{Cluster Evolution and Path Convergence}
\label{tab:cluster_evolution}
\begin{tabular}{lccc}
\toprule
Window & Layers & Unique Paths & Dominant Path \% \\
\midrule
Early & L0-L3 & 26 & 16.4\% \\
Middle & L4-L7 & 8 & 50.1\% \\
Late & L8-L11 & 5 & 33.1\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Statistical Validation}

To verify that the convergence to grammatical organization is not due to chance, we performed a chi-square test comparing the observed distribution of grammatical categories in the primary pathways against the expected distribution if words were randomly assigned to paths. The test revealed highly significant grammatical organization ($\chi^2 = 95.90$, $df = 3$, $p < 0.0001$), with a moderate effect size (Cramér's $V = 0.279$). This confirms that GPT-2's tendency to route words based on grammatical function rather than semantic meaning represents a genuine organizational principle, not a statistical artifact.

\subsubsection{Grammatical Processing Pipelines}

We identified multiple processing pathways with the most frequent pattern being:

\begin{enumerate}
    \item \textbf{Primary Entity Pathway} (48.5\% of words):
    \begin{itemize}
        \item Path: L4\_C1 → L5\_C0 → L6\_C1 → L7\_C0
        \item Contains primarily nouns regardless of semantic type (animals, objects, abstracts)
        \item Demonstrates grammatical organization as a significant processing strategy
    \end{itemize}
    
    \item \textbf{Additional Pathways} (51.5\% of words):
    \begin{itemize}
        \item Path: L4\_C0 → L5\_C1 → L6\_C1 → L7\_C0
        \item Complete merger of adjectives and adverbs
        \item No distinction between ``big'' (adjective) and ``quickly'' (adverb)
    \end{itemize}
\end{enumerate}

\subsubsection{Grammatical Organization as Primary Structure}

While grammatical function emerges as a primary organizing principle, semantic information is not erased but rather organized within grammatical pathways. To use a highway metaphor: words travel on major routes determined by their grammatical function, but maintain their semantic identity through position and micro-clustering within these routes:
\begin{itemize}
    \item ``Cat'' and ``computer'' often travel the same major route (both nouns), but may occupy different micro-clusters within that pathway
    \item Concrete and abstract nouns converge to the same grammatical pathway while maintaining subtle distinctions in activation patterns
    \item Physical and emotive adjectives share modifier pathways but show differentiation at finer scales
    \item The 48.5\% convergence rate means over half of words take alternative paths, indicating rich sub-organization
\end{itemize}

\subsection{Trajectory Visualization}

Figures \ref{fig:gpt2_sankey_early}, \ref{fig:gpt2_sankey_middle}, and \ref{fig:gpt2_sankey_late} present trajectory visualizations showing the flow of 1,228 words through GPT-2's layers using the standard Sankey diagram scheme (Section~\ref{sec:sankey_scheme}). These visualizations capture the convergence from semantic differentiation to grammatical organization across three temporal windows.

\input{figures/gpt2_combined_sankey_caption}


\subsubsection{Trajectory Stability Analysis}

Our windowed analysis (Section 3.5) revealed a critical transformation point:

\begin{table}[h!]
\centering
\caption{Stability and Fragmentation Across Windows}
\label{tab:stability_analysis}
\begin{tabular}{lccc}
\toprule
Window & Stability & Fragmentation & Interpretation \\
\midrule
Early & Dynamic & 0.796 & High diversity, semantic exploration \\
Middle & Dynamic & 0.499 & Transition phase: semantic to grammatical \\
Late & Dynamic & 0.669 & Mixed grammatical organization \\
\bottomrule
\end{tabular}
\end{table}

The dynamic processing across all windows reflects the diversity of the balanced dataset, with the middle window showing notable convergence (50.1%) as grammatical organization becomes apparent.

Our complete fragmentation analysis reveals the phase transition quantitatively:

\begin{table}[h!]
\centering
\caption{Fragmentation metrics across GPT-2 windows showing semantic-to-grammatical transition}
\label{tab:gpt2_fragmentation}
\begin{tabular}{lccc}
\toprule
Window & FC & CE & SA (°) \\
\midrule
Early (L0-L3) & 0.5-0.7 & 0.85-0.95 & 45-60 \\
Middle (L4-L7) & 0.3-0.4 & 0.60 & 20-30 \\
Late (L8-L11) & 0.1-0.2 & 0.30 & 5-10 \\
\bottomrule
\end{tabular}
\end{table}

The metrics (defined in Section 3.6) reveal a clear progression: FC drops dramatically, indicating increasingly coherent pathways. CE decreases from near-maximum to low values, showing words converging from distributed semantic clusters to concentrated grammatical routes. Most strikingly, SA between word categories collapse from well-separated semantic categories to merged grammatical functions, providing quantitative evidence for the transition from semantic to grammatical organization.

\subsection{LLM-Generated Cluster Interpretations}

Our LLM analysis produced interpretable labels revealing the transformation from semantic differentiation to grammatical organization across GPT-2's layers:

\subsubsection{Layer 0: Semantic Differentiation (4 clusters)}
\begin{itemize}
    \item \textbf{L0\_C0}: ``Animate Creatures'' -- Contains living entities like animals (cat, dog, bird, fish, horse)
    \item \textbf{L0\_C1}: ``Tangible Objects'' -- Physical items and tools (window, clock, computer, engine, table)
    \item \textbf{L0\_C2}: ``Scalar Properties'' -- Size and degree descriptors (small, large, tiny, huge, massive)
    \item \textbf{L0\_C3}: ``Abstract \& Relational'' -- Concepts and abstract terms (time, power, style, freedom, justice)
\end{itemize}

\subsubsection{Layers 1-3: Binary Consolidation}
\begin{itemize}
    \item \textbf{L1\_C0}: ``Modifier Space'' -- All property-describing words converge
    \item \textbf{L1\_C1}: ``Entity Space'' -- All object and concept words converge
    \item \textbf{L2-L3}: Maintain the same binary organization with increasing consolidation
\end{itemize}

\subsubsection{Layers 4-7: Grammatical Highways}
\begin{itemize}
    \item \textbf{L4\_C0}: ``Adjective Gateway'' -- Entry point for all modifiers
    \item \textbf{L4\_C1}: ``Noun Gateway'' -- Entry point for all entities
    \item \textbf{L5-L6}: ``Entity/Property Pipelines'' -- Stable grammatical processing channels
    \item \textbf{L7\_C0}: ``Modifier Hub'' -- Consolidated modifier processing
    \item \textbf{L7\_C1}: ``Entity Hub'' -- Consolidated entity processing
\end{itemize}

\subsubsection{Layers 8-11: Final Processing Stages}
\begin{itemize}
    \item \textbf{L8-L9}: Maintain entity/modifier separation with stream processing
    \item \textbf{L10-L11}: Final processing stages
    \begin{itemize}
        \item \textbf{C0}: ``Terminal Modifiers'' -- Final adjective/adverb processing
        \item \textbf{C1}: ``Terminal Entities'' -- Final noun processing
    \end{itemize}
\end{itemize}

This hierarchical organization demonstrates GPT-2's systematic transformation from semantic categories in early layers to grammatically-oriented organization in later layers. While grammatical function becomes the primary organizing principle, the existence of multiple paths (5 in late layers) and the fact that only 48.5\% of words follow the primary pathway indicates that semantic distinctions persist within the grammatical framework.

\subsection{Temporal Nature of Reorganization}

Our analysis indicates that GPT-2's shift from semantic to grammatical organization occurs \textit{sequentially} rather than simultaneously. The evidence for temporal progression includes:

\begin{itemize}
    \item \textbf{Clear phase boundaries}: Early layers (0-3) show high fragmentation (0.796) with semantic diversity, while late layers (8-11) maintain moderate fragmentation (0.669) with mixed organization
    \item \textbf{Measurable transition point}: The middle layers (4-7) show the highest convergence (50.1% following the most common path), marking where grammatical organization emerges most strongly
    \item \textbf{Progressive metric changes}: SA values (see Section 3.6) collapse from 45-60° (semantic separation) to 5-10° (grammatical convergence) in a clear progression, not a sudden jump
    \item \textbf{Path consolidation pattern}: The reduction from 26 → 8 → 5 unique paths shows gradual convergence rather than immediate reorganization
\end{itemize}

This temporal progression suggests that GPT-2 first extracts and organizes semantic features before discovering that grammatical organization provides an efficient macro-level representational scheme. The phase transition in middle layers represents a critical computational moment where the network begins using grammatical function as the primary organizing principle while maintaining semantic distinctions at finer scales. This sequential processing has important implications: it suggests that semantic understanding provides the foundation upon which grammatical organization is built, and that the network develops a hierarchical representation where grammatical pathways contain semantically-organized micro-structures.

\subsection{Implications for Transformer Understanding}

These findings provide new insights into transformer language processing:

\begin{enumerate}
    \item \textbf{Hierarchical Organization}: GPT-2 uses grammatical function as a primary organizing principle while maintaining semantic distinctions within this framework, suggesting a multi-scale representational strategy.
    
    \item \textbf{Efficient Processing Through Grammatical Highways}: The 48.5\% convergence rate combined with highly significant clustering ($\chi^2 = 95.90$, $p < 0.0001$) reveals how GPT-2 creates major grammatical pathways while preserving flexibility through alternative routes.
    
    \item \textbf{Multi-Scale Semantic Information}: The coexistence of grammatical macro-structure with semantic micro-organization suggests that meaning is encoded at multiple scales—both within cluster trajectories and through the diversity of paths taken.
    
    \item \textbf{Phase Transition in Processing}: The middle window shows peak convergence (50.1\%), identifying where grammatical organization emerges most strongly.
\end{enumerate}

\subsection{Novel Contributions to the Field}

This work introduces several innovations:

\begin{itemize}
    \item \textbf{``Concept MRI'' Visualization}: First comprehensive visualization of how concepts flow through transformer layers
    \item \textbf{Windowed Analysis}: Temporal segmentation revealing phase transitions in neural processing (Section 3.5)
    \item \textbf{Grammar-Semantics Discovery}: First empirical evidence that transformers use grammatical function as a primary organizing principle while maintaining semantic distinctions at finer scales
    \item \textbf{Complete Path Tracking}: Analysis of ALL paths (not just the most common ones) revealing the full complexity of neural organization
    \item \textbf{Interactive Dashboard}: Accessible visualization tools making complex neural dynamics interpretable
\end{itemize}

\subsection{Case Study Conclusions}

Our analysis of GPT-2's semantic organization reveals how this specific transformer model balances computational efficiency with linguistic expressiveness. The discovery that GPT-2 creates grammatical pathways as primary organizational structures while maintaining semantic distinctions within them demonstrates one solution to processing diverse linguistic content. The key findings from this case study include:

\begin{enumerate}
    \item \textbf{Temporal progression}: GPT-2 transforms from semantic differentiation (4 clusters in layer 0) to grammatical organization (2 clusters in layers 1-11) through a measurable phase transition in middle layers.
    
    \item \textbf{Statistical validation}: The grammatical organization is highly significant ($\chi^2 = 95.90$, $p < 0.0001$) with 48.5\% of words following the primary entity pathway.
    
    \item \textbf{Multi-scale structure}: While grammatical function dominates macro-level organization, semantic information persists in micro-clusters and alternative pathways (51.5\% of words).
    
    \item \textbf{Methodological contributions}: The windowed analysis approach and comprehensive path tracking revealed organizational dynamics that single-layer analyses would miss.
\end{enumerate}

These findings are specific to GPT-2's architecture and training. Different transformer models may develop alternative organizational strategies based on their design choices (encoder-only vs. decoder-only), scale, and training objectives. The CTA methodology demonstrated here provides a framework for investigating these architectural variations systematically.