\section{GPT-2 Semantic Subtypes Case Study: Revealing Grammatical Organization Using CTA}

Where medical AI models organize patients by clinical risk factors, language models face a different challenge: how to organize the vast complexity of human language. We present a comprehensive analysis revealing that GPT-2 organizes language by grammatical function rather than semantic meaning—a finding that challenges conventional assumptions about neural language models. Using CTA within a Concept MRI tool, we tracked 1,228 validated single-token words across 8 semantic categories through GPT-2's layers, discovering a convergence from semantic differentiation to grammatical organization.

\subsection{Experimental Design}

We designed a systematic experiment to study how GPT-2 organizes semantic knowledge using 1,228 validated single-token words across 8 semantically distinct categories with balanced grammatical representation:

\begin{itemize}
    \item \textbf{Concrete Nouns}: Physical objects (e.g., ``table'', ``mountain'', ``book'')
    \item \textbf{Abstract Nouns}: Conceptual entities (e.g., ``freedom'', ``justice'', ``emotion'')
    \item \textbf{Physical Adjectives}: Observable properties (e.g., ``tall'', ``smooth'', ``bright'')
    \item \textbf{Emotive Adjectives}: Emotional descriptors (e.g., ``joyful'', ``melancholy'', ``serene'')
    \item \textbf{Manner Adverbs}: How actions are performed (e.g., ``quickly'', ``carefully'', ``boldly'')
    \item \textbf{Degree Adverbs}: Intensity modifiers (e.g., ``extremely'', ``barely'', ``quite'')
    \item \textbf{Action Verbs}: Dynamic processes (e.g., ``run'', ``create'', ``destroy'')
    \item \textbf{Stative Verbs}: State descriptions (e.g., ``exist'', ``belong'', ``resemble'')
\end{itemize}

\subsubsection{Dataset Construction}

We curated 1,228 validated single-token words distributed across semantic subtypes through systematic linguistic analysis, achieving balanced grammatical representation: 275 nouns (22.4%), 280 adjectives (22.8%), 267 adverbs (21.7%), and 406 verbs (33.1%). Each word was verified for single-token representation in GPT-2's tokenizer and semantic category membership, ensuring clean experimental conditions for concept trajectory analysis.

\subsubsection{Novel Methodological Innovations}

Our analysis introduced several key innovations:

\begin{itemize}
    \item \textbf{Unified CTA with Gap Statistic}: Optimal k determination using Gap statistic (k=4 for layer 0, k=2 for layers 1-11)
    \item \textbf{Windowed Analysis}: Novel temporal segmentation into Early (L0-L3), Middle (L4-L7), and Late (L8-L11) windows
    \item \textbf{Unique Cluster Labeling}: Every cluster assigned unique ID (e.g., L4\_C1) to prevent cross-layer confusion
    \item \textbf{``Concept MRI'' Visualization}: Interactive Sankey diagrams showing complete concept flow through network
    \item \textbf{Comprehensive Path Analysis}: Tracking ALL paths (not just archetypal), revealing 26→8→5 path convergence
\end{itemize}

\subsection{Key Findings}

\subsubsection{The Grammar-Semantics Inversion}

Our most striking discovery: GPT-2 organizes words by grammatical function, not semantic meaning. The evidence is compelling:

\begin{itemize}
    \item \textbf{Layer 0}: 4 clusters showing semantic differentiation (animals, objects, properties, abstracts)
    \item \textbf{Layers 1-11}: Rapid consolidation to just 2 clusters (entities vs. modifiers)
    \item \textbf{Convergence rate}: 48.5\% (95\% CI: 45.7\%–51.2\%) of all words converge to a single ``entity superhighway''
    \item \textbf{Path reduction}: 26 unique paths → 8 paths → 5 paths across windows
\end{itemize}

\begin{table}[h!]
\centering
\caption{Cluster Evolution and Path Convergence}
\label{tab:cluster_evolution}
\begin{tabular}{lccc}
\toprule
Window & Layers & Unique Paths & Dominant Path \% \\
\midrule
Early & L0-L3 & 26 & 16.4\% \\
Middle & L4-L7 & 8 & 50.1\% \\
Late & L8-L11 & 5 & 33.1\% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Statistical Validation}

To verify that the convergence to grammatical organization is not due to chance, we performed a chi-square test comparing the observed distribution of grammatical categories in the dominant pathways against the expected distribution if words were randomly assigned to paths. The test revealed highly significant grammatical organization ($\chi^2 = 95.90$, $df = 3$, $p < 0.0001$), with a moderate effect size (Cramér's $V = 0.279$). This confirms that GPT-2's tendency to route words based on grammatical function rather than semantic meaning represents a genuine organizational principle, not a statistical artifact.

\subsubsection{Grammatical Processing Pipelines}

We identified multiple processing pathways with the dominant pattern being:

\begin{enumerate}
    \item \textbf{Entity Superhighway} (48.5\% of words):
    \begin{itemize}
        \item Path: L4\_C1 → L5\_C0 → L6\_C1 → L7\_C0
        \item Contains primarily nouns regardless of semantic type (animals, objects, abstracts)
        \item Demonstrates grammatical organization as the dominant processing strategy
    \end{itemize}
    
    \item \textbf{Additional Pathways} (51.5\% of words):
    \begin{itemize}
        \item Path: L4\_C0 → L5\_C1 → L6\_C1 → L7\_C0
        \item Complete merger of adjectives and adverbs
        \item No distinction between ``big'' (adjective) and ``quickly'' (adverb)
    \end{itemize}
\end{enumerate}

\subsubsection{Grammatical Organization as Primary Structure}

While grammatical function emerges as the dominant organizing principle, semantic information is not erased but rather organized within grammatical highways:
\begin{itemize}
    \item ``Cat'' and ``computer'' often travel the same major highway (both nouns), but may occupy different micro-clusters within that highway
    \item Concrete and abstract nouns converge to the same grammatical pathway while maintaining subtle distinctions in activation patterns
    \item Physical and emotive adjectives share modifier pathways but show differentiation at finer scales
    \item The 48.5\% convergence rate means over half of words take alternative paths, indicating rich sub-organization
\end{itemize}

\subsection{Concept MRI Visualization}

Figure \ref{fig:gpt2_concept_mri} presents the Concept MRI tool's output showing the complete flow of 1,228 words through GPT-2's layers as analyzed by CTA. The Sankey diagrams reveal the convergence from semantic differentiation to grammatical organization across three windows.

\input{figures/gpt2_combined_sankey_caption}

\subsubsection{3D Stepped-Layer Visualization}

To complement the Sankey diagrams, we visualize word trajectories through GPT-2's representation space using stepped-layer visualization. These 3D visualizations reveal the dramatic transformation from semantic to grammatical organization, with each layer positioned at a different height to show progression through the network:

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/gpt2_stepped_layer_early.png}
        \caption{Early Window (L0-L3)}
        \label{fig:gpt2_traj_early}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/gpt2_stepped_layer_middle.png}
        \caption{Middle Window (L4-L7)}
        \label{fig:gpt2_traj_middle}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.32\textwidth}
        \includegraphics[width=\textwidth]{figures/gpt2_stepped_layer_late.png}
        \caption{Late Window (L8-L11)}
        \label{fig:gpt2_traj_late}
    \end{subfigure}
    \caption{GPT-2 stepped-layer trajectory visualizations across three windows (n=1,228). Each layer is positioned at a different height showing flow through the network. (a) Early window shows 26 diverse paths with high semantic diversity. (b) Middle window reveals convergence to 8 paths as grammatical organization emerges, with 50.1\% following the dominant pathway. (c) Late window shows 5 consolidated pathways reflecting mixed grammatical organization. The progression from semantic diversity to grammatical organization is clearly visible in the trajectory patterns.}
    \label{fig:gpt2_trajectories}
\end{figure}

\subsubsection{Trajectory Stability Analysis}

Our windowed analysis revealed a critical transformation point:

\begin{table}[h!]
\centering
\caption{Stability and Fragmentation Across Windows}
\label{tab:stability_analysis}
\begin{tabular}{lccc}
\toprule
Window & Stability & Fragmentation & Interpretation \\
\midrule
Early & Dynamic & 0.796 & High diversity, semantic exploration \\
Middle & Dynamic & 0.499 & Transition phase: semantic to grammatical \\
Late & Dynamic & 0.669 & Mixed grammatical organization \\
\bottomrule
\end{tabular}
\end{table}

The dynamic processing across all windows reflects the increased diversity of the balanced dataset, with the middle window showing the highest convergence (50.1%) as grammatical organization emerges.

Our complete fragmentation analysis reveals the phase transition quantitatively:

\begin{table}[h!]
\centering
\caption{Fragmentation metrics across GPT-2 windows showing semantic-to-grammatical transition}
\label{tab:gpt2_fragmentation}
\begin{tabular}{lccc}
\toprule
Window & FC & CE & SA (°) \\
\midrule
Early (L0-L3) & 0.5-0.7 & 0.85-0.95 & 45-60 \\
Middle (L4-L7) & 0.3-0.4 & 0.60 & 20-30 \\
Late (L8-L11) & 0.1-0.2 & 0.30 & 5-10 \\
\bottomrule
\end{tabular}
\end{table}

Path-Centroid Fragmentation (FC) drops dramatically, indicating increasingly coherent pathways. Intra-Class Cluster Entropy (CE) decreases from near-maximum to low values, showing words converging from distributed semantic clusters to concentrated grammatical highways. Most strikingly, Sub-space Angles (SA) between word categories collapse from well-separated semantic categories to merged grammatical functions, providing mathematical evidence for the grammar-semantics inversion.

\subsection{LLM-Generated Cluster Interpretations}

Our LLM analysis produced interpretable labels revealing the transformation from semantic differentiation to grammatical organization across GPT-2's layers:

\subsubsection{Layer 0: Semantic Differentiation (4 clusters)}
\begin{itemize}
    \item \textbf{L0\_C0}: ``Animate Creatures'' -- Contains living entities like animals (cat, dog, bird, fish, horse)
    \item \textbf{L0\_C1}: ``Tangible Objects'' -- Physical items and tools (window, clock, computer, engine, table)
    \item \textbf{L0\_C2}: ``Scalar Properties'' -- Size and degree descriptors (small, large, tiny, huge, massive)
    \item \textbf{L0\_C3}: ``Abstract \& Relational'' -- Concepts and abstract terms (time, power, style, freedom, justice)
\end{itemize}

\subsubsection{Layers 1-3: Binary Consolidation}
\begin{itemize}
    \item \textbf{L1\_C0}: ``Modifier Space'' -- All property-describing words converge
    \item \textbf{L1\_C1}: ``Entity Space'' -- All object and concept words converge
    \item \textbf{L2-L3}: Maintain the same binary organization with increasing consolidation
\end{itemize}

\subsubsection{Layers 4-7: Grammatical Highways}
\begin{itemize}
    \item \textbf{L4\_C0}: ``Adjective Gateway'' -- Entry point for all modifiers
    \item \textbf{L4\_C1}: ``Noun Gateway'' -- Entry point for all entities
    \item \textbf{L5-L6}: ``Entity/Property Pipelines'' -- Stable grammatical processing channels
    \item \textbf{L7\_C0}: ``Modifier Hub'' -- Consolidated modifier processing
    \item \textbf{L7\_C1}: ``Entity Hub'' -- Consolidated entity processing
\end{itemize}

\subsubsection{Layers 8-11: Syntactic Superhighways}
\begin{itemize}
    \item \textbf{L8-L9}: Maintain entity/modifier separation with stream processing
    \item \textbf{L10-L11}: Final processing stages
    \begin{itemize}
        \item \textbf{C0}: ``Terminal Modifiers'' -- Final adjective/adverb processing
        \item \textbf{C1}: ``Terminal Entities'' -- Final noun processing
    \end{itemize}
\end{itemize}

This hierarchical organization demonstrates GPT-2's systematic transformation from semantic categories in early layers to grammatical-dominant organization in later layers. While grammatical function becomes the primary organizing principle, the existence of multiple paths (5 in late layers) and the fact that only 48.5\% of words follow the dominant highway indicates that semantic distinctions persist within the grammatical framework.

\subsection{Temporal Nature of Reorganization}

A critical aspect of our findings is that GPT-2's shift from semantic to grammatical organization occurs \textit{sequentially} rather than simultaneously. The evidence for temporal progression is compelling:

\begin{itemize}
    \item \textbf{Clear phase boundaries}: Early layers (0-3) show high fragmentation (0.796) with semantic diversity, while late layers (8-11) maintain moderate fragmentation (0.669) with mixed organization
    \item \textbf{Measurable transition point}: The middle layers (4-7) show the highest convergence (50.1% following dominant path), marking where grammatical organization emerges most strongly
    \item \textbf{Progressive metric changes}: Sub-space angles collapse from 45-60° (semantic separation) to 5-10° (grammatical convergence) in a clear progression, not a sudden jump
    \item \textbf{Path consolidation pattern}: The reduction from 26 → 8 → 5 unique paths shows gradual convergence rather than immediate reorganization
\end{itemize}

This temporal progression suggests that GPT-2 first extracts and organizes semantic features before discovering that grammatical organization provides an efficient macro-level representational scheme. The phase transition in middle layers represents a critical computational moment where the network begins using grammatical function as the primary organizing principle while maintaining semantic distinctions at finer scales. This sequential processing has important implications: it suggests that semantic understanding provides the foundation upon which grammatical organization is built, and that the network develops a hierarchical representation where grammatical highways contain semantically-organized micro-structures.

\subsection{Implications for Transformer Understanding}

Our findings fundamentally challenge how we think about language models:

\begin{enumerate}
    \item \textbf{Hierarchical Organization}: GPT-2 uses grammatical function as a primary organizing principle while maintaining semantic distinctions within this framework, suggesting a multi-scale representational strategy.
    
    \item \textbf{Efficient Processing Through Grammatical Highways}: The 48.5\% convergence rate combined with highly significant clustering ($\chi^2 = 95.90$, $p < 0.0001$) reveals how GPT-2 creates major grammatical pathways while preserving flexibility through alternative routes.
    
    \item \textbf{Multi-Scale Semantic Information}: The coexistence of grammatical macro-structure with semantic micro-organization suggests that meaning is encoded at multiple scales—both within cluster trajectories and through the diversity of paths taken.
    
    \item \textbf{Phase Transition in Processing}: The middle window shows peak convergence (50.1\%), identifying where grammatical organization emerges most strongly.
\end{enumerate}

\subsection{Novel Contributions to the Field}

This work introduces several innovations:

\begin{itemize}
    \item \textbf{``Concept MRI'' Visualization}: First comprehensive visualization of how concepts flow through transformer layers
    \item \textbf{Windowed Analysis}: Novel temporal segmentation revealing phase transitions in neural processing
    \item \textbf{Grammar-Semantics Discovery}: First empirical evidence that transformers use grammatical function as a primary organizing principle while maintaining semantic distinctions at finer scales
    \item \textbf{Complete Path Tracking}: Analysis of ALL paths (not just dominant ones) revealing the full complexity of neural organization
    \item \textbf{Interactive Dashboard}: Accessible visualization tools making complex neural dynamics interpretable
\end{itemize}

\subsection{Conclusion}

The GPT-2 semantic subtypes analysis reveals a profound insight: neural language models develop hierarchical organization strategies that balance efficiency with expressiveness. By creating grammatical highways as primary organizational structures while maintaining semantic distinctions within them, GPT-2 achieves both computational efficiency and representational richness. This multi-scale organization—grammatical at the macro level, semantic at the micro level—demonstrates how transformers solve the challenge of processing diverse linguistic content. This discovery, enabled by CTA analysis within the Concept MRI tool, suggests that effective language models must balance structure with flexibility, using grammatical organization to create efficient pathways while preserving the semantic nuances necessary for understanding.