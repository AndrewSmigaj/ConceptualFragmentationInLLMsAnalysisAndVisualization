\section{Introduction}

Understanding how neural networks organize linguistic information remains a fundamental challenge in interpretability research. We present Concept Trajectory Analysis (CTA), a method that tracks concept evolution through neural networks by analyzing movement patterns in clustered activation spaces. Application of this method to GPT-2 indicates that the model organizes words primarily by grammatical function rather than semantic similarity, providing new insights into transformer language processing.

\begin{table}[h!]
\centering
\caption{Key Terminology Used in This Paper}
\label{tab:glossary}
\begin{tabular}{ll}
\toprule
\textbf{Term} & \textbf{Definition} \\
\midrule
Trajectory & The path a concept takes through cluster assignments across layers \\
Path & A specific sequence of clusters, e.g., L1\_C0→L2\_C1→L3\_C0 \\
Highway & A meso-level bundle of similar paths traveled by many concepts \\
Window & Temporal grouping of layers (Early: L0-3, Middle: L4-7, Late: L8-11) \\
Fragmentation (F) & Metric measuring diversity of trajectories within a concept group \\
FC & Path-Centroid Fragmentation: distance between paths and group centroid \\
CE & Intra-Class Cluster Entropy: distribution of concepts across clusters \\
SA & Sub-space Angle: angular separation between concept group representations \\
Microcluster & Fine-grained sub-structure within larger cluster (future work) \\
\bottomrule
\end{tabular}
\end{table}

In our analysis of 1,228 single-token words across 8 semantic categories with balanced grammatical distribution (33.1\% verbs), we observed a temporal progression: words initially cluster by semantic similarity in early layers, undergo a phase transition in middle layers, and ultimately converge to grammatical organization in later layers. By the final layers, 48.5\% (95\% CI: 45.7\%–51.2\%) of words follow the most common pathway, while others distribute across multiple pathways. This sequential reorganization—validated by highly significant grammatical clustering ($\chi^2 = 95.90$, $p < 0.0001$)—suggests that neural networks first process semantic features before reorganizing by grammatical function.

This observation emerged from our framework that combines mathematical analysis with human interpretation. CTA tracks concepts through clustered activation spaces across neural network layers, using quantitative metrics alongside LLM-generated explanations. The method addresses several questions in interpretability research:

\begin{itemize}
    \item How can we make neural organization both mathematically precise and humanly interpretable?
    \item What organizational principles do neural networks discover that differ from human intuition?
    \item Can we create real-time interpretability with negligible computational overhead?
    \item How do we scale from analyzing hundreds to millions of concepts?
\end{itemize}

We validate CTA through experiments on both language models and traditional ML. Our windowed analysis (Early/Middle/Late) identifies phase transitions in neural processing, while unique cluster labeling (L\{layer\}\_C\{cluster\}) enables precise tracking. By analyzing all trajectories rather than just dominant paths, we capture the full complexity of neural organization.

Beyond transformer analysis, CTA can be applied to traditional ML tasks. In heart disease diagnosis, trajectory analysis shows how neural networks process patient data, with different pathways emerging for various patient profiles. High-risk patients (older, elevated cholesterol) tend to route through specific neural pathways with higher fragmentation scores, potentially indicating model uncertainty. This medical AI application illustrates how CTA can help understand decision-making processes in healthcare domains.

\subsection{Background: Concept Trajectory Analysis}

Concept Trajectory Analysis (CTA) clusters datapoint activations in each layer's activation space (e.g., using k-means), assigning layer-specific cluster IDs denoted L$l$\_C$k$, where $l$ is the layer index and $k$ is the cluster index. Transitions between clusters are tracked across layers, forming trajectories $\pi_i = [c_i^1, c_i^2, \dots, c_i^L]$, interpreted as concept evolution through the network. In feedforward networks, trajectories are strictly unidirectional, and clusters with different layer-specific IDs (e.g., L1\_C0 and L3\_C0) are not assumed related unless validated by geometric similarity metrics. Large language models can then narrate these trajectories to provide interpretable insights.

While activation spaces are emergent, high-dimensional representations whose coordinate systems may not map to semantically meaningful axes \citep{ribeiro2016, lundberg2017}, CTA addresses these concerns through rigorous mathematical validation and cross-layer metrics that verify genuine patterns rather than artifacts.

\textbf{Contributions}:
\begin{itemize}
    \item Formalize Concept Trajectory Analysis with mathematical validation criteria and cross-layer metrics
    \item Develop optimal clustering determination using Gap statistic with layer-specific cluster counts
    \item Implement trajectory visualization and LLM-powered interpretability for neural network analysis
    \item Provide evidence that GPT-2 organizes words primarily by grammatical function in later layers
    \item Demonstrate CTA's applicability to both transformer models and traditional ML tasks
    \item Release open-source implementation for reproducible neural network interpretability research
\end{itemize}