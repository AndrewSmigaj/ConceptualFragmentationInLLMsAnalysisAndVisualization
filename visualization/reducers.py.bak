"""
Dimensionality reduction module for the concept fragmentation visualization.

This module contains functionality for reducing the dimensionality of
neural network layer activations using UMAP, with disk caching to
avoid recomputing embeddings.
"""

import os
import sys
import hashlib
import pickle
import argparse
import numpy as np
from typing import Dict, List, Tuple, Any, Optional, Union
from pathlib import Path

try:
    import umap
    from umap import UMAP
except ImportError:
    print("UMAP not installed. Install with: pip install umap-learn")
    UMAP = None

# Add parent directory to path to import data_interface
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if parent_dir not in sys.path:
    sys.path.insert(0, parent_dir)

from visualization.data_interface import load_activations, load_stats, get_best_config, get_baseline_config

class Embedder:
    """
    Class for dimensionality reduction of layer activations.
    Handles UMAP embedding with consistent parameters and disk caching.
    """
    
    def __init__(self, 
                 n_components: int = 3, 
                 n_neighbors: int = 15, 
                 min_dist: float = 0.1,
                 metric: str = "euclidean",
                 random_state: int = 42,
                 cache_dir: Optional[str] = None):
        """
        Initialize the embedder.
        
        Args:
            n_components: Number of dimensions to reduce to.
            n_neighbors: Number of neighbors for UMAP.
            min_dist: Minimum distance for UMAP.
            metric: Distance metric for UMAP.
            random_state: Random state for reproducibility.
            cache_dir: Directory to store cached embeddings.
        """
        self.n_components = n_components
        self.n_neighbors = n_neighbors
        self.min_dist = min_dist
        self.metric = metric
        self.random_state = random_state
        
        # Set up cache directory
        if cache_dir is None:
            cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "cache")
        self.cache_dir = cache_dir
        
        # Ensure cache directory exists
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # Initialize UMAP
        if UMAP is not None:
            self.umap = UMAP(
                n_components=n_components,
                n_neighbors=n_neighbors,
                min_dist=min_dist,
                metric=metric,
                random_state=random_state
            )
        else:
            self.umap = None
    
    def _get_cache_path(self, data: np.ndarray) -> str:
        """
        Get the cache path for the given data.
        
        Args:
            data: Input data array.
            
        Returns:
            Path to the cache file.
        """
        # Add diagnostic prints to identify problematic data
        print(f"DEBUG _get_cache_path: type={type(data)}")
        if not isinstance(data, np.ndarray):
            print(f"WARNING: data is not ndarray but {type(data)}")
            if isinstance(data, list):
                print(f"  List length: {len(data)}")
                if data:
                    print(f"  First element type: {type(data[0])}")
        
        # Ensure we have a proper numeric array that supports .size
        if not isinstance(data, np.ndarray):
            print(f"Warning: Converting non-ndarray of type {type(data)} to numpy array")
            data = np.asarray(data)
        
        # Handle object-dtype arrays that might contain lists
        if data.dtype == object:
            print(f"Warning: Converting object-dtype array to numeric array")
            try:
                # Try to stack into a 2D array if all elements are same length
                data = np.vstack(data)
            except Exception as e:
                # Fall back: flatten everything into 1D array
                try:
                    data = np.hstack([np.asarray(x).ravel() for x in data])
                except Exception as e2:
                    print(f"Warning: Could not convert object array: {e2}")
                    # Last resort - just use a small sample that won't crash
                    data = np.zeros((10, 10))
        
        # Create a hash of the data and parameters
        # We only hash a subset of the data to avoid memory issues with large arrays
        # and still have a reasonable chance of detecting changes
        data_sample = data
        if data.size > 1000:
            indices = np.linspace(0, data.size - 1, 1000, dtype=int)
            data_sample = data.ravel()[indices]
        
        params_str = f"{self.n_components}_{self.n_neighbors}_{self.min_dist}_{self.metric}_{self.random_state}"
        hash_input = str(data_sample.shape) + str(data_sample.mean()) + str(data_sample.std()) + params_str
        
        # Create a hash of the input
        hash_obj = hashlib.md5(hash_input.encode())
        hash_str = hash_obj.hexdigest()
        
        # Return the cache path
        return os.path.join(self.cache_dir, f"embedding_{hash_str}.npz")
    
    def fit_transform(self, data: np.ndarray) -> np.ndarray:
        """
        Apply dimensionality reduction to the data.
        
        Args:
            data: Input data array.
            
        Returns:
            Embedded data in lower-dimensional space.
        """
        if self.umap is None:
            raise ImportError("UMAP is not installed. Install with: pip install umap-learn")
        
        # Check if we have a cached result
        cache_path = self._get_cache_path(data)
        if os.path.exists(cache_path):
            print(f"Loading cached embedding from {cache_path}")
            try:
                with np.load(cache_path) as npz:
                    embedding = npz["embedding"]
                return embedding
            except Exception as e:
                print(f"Error loading cache: {e}")
                print("Recomputing embedding...")
        
        # Apply UMAP
        print(f"Computing UMAP embedding for data with shape {data.shape}...")
        embedding = self.umap.fit_transform(data)
        
        # Cache the result
        print(f"Caching embedding to {cache_path}")
        np.savez_compressed(cache_path, embedding=embedding)
        
        return embedding

def embed_layer_activations(    dataset: str,     config: Dict[str, Any],     seed: int,    layers: Optional[List[str]] = None,    embedder: Optional[Embedder] = None,    use_test_set: bool = True,  # Whether to use test or train set    epoch_idx: int = -1  # Which epoch to use (-1 means last epoch)) -> Dict[str, np.ndarray]:    """    Embed layer activations for the given dataset, configuration, and seed.        Args:        dataset: Name of the dataset.        config: Configuration dictionary.        seed: Random seed.        layers: List of layer names to embed. If None, embed all layers.        embedder: Embedder instance. If None, create a new one.        use_test_set: Whether to use test set (True) or train set (False).        epoch_idx: Which epoch to use (-1 for last epoch).            Returns:        Dictionary mapping layer names to embedded activations.    """
    # Load activations
    activations = load_activations(dataset, config, seed)
    
    # Add detailed diagnostic logging
    print(f"\n=== Embedding Layer Activations ===")
    print(f"Dataset: {dataset}")
    if isinstance(config, dict):
        print(f"Config: {config}")
    else:
        print(f"Config: {config}")
    print(f"Seed: {seed}")
    print(f"Using {'test' if use_test_set else 'train'} set")
    
    print(f"\nAvailable keys:")
    for key, value in activations.items():
        if isinstance(value, np.ndarray):
            print(f"  '{key}': numpy array with shape {value.shape}, dtype {value.dtype}")
        elif isinstance(value, dict) and 'test' in value and 'train' in value:
            test_val = value['test']
            train_val = value['train']
            print(f"  '{key}': dict with test/train, test shape: {type(test_val)}")
        else:
            print(f"  '{key}': {type(value)}")
    
    # Create embedder if not provided
    if embedder is None:
        embedder = Embedder()
    
    # Filter layers if specified
    layer_keys = [k for k in activations.keys() if k.startswith('layer') and isinstance(activations[k], dict)]
    if layers is not None:
        layer_keys = [k for k in layer_keys if k in layers]
    
    print(f"\nProcessing layer keys: {layer_keys}")
    
    # Extract the appropriate data (test/train) from each layer
    valid_activations = {}
    for layer in layer_keys:
        layer_data = activations[layer]
        if not isinstance(layer_data, dict) or 'test' not in layer_data or 'train' not in layer_data:
            print(f"Skipping layer '{layer}': unexpected format (not a dict with test/train)")
            continue
        
        # Get the appropriate split
        split_key = 'test' if use_test_set else 'train'
        data = layer_data[split_key]
        
        # Convert list to numpy array if needed
        if isinstance(data, list):
            print(f"Converting {layer} {split_key} list to numpy array...")
            try:
                data = np.array(data)
                print(f"Successfully converted to shape {data.shape}")
            except Exception as e:
                print(f"Failed to convert list to array: {e}")
                continue
        
        # Validate the data
        if not isinstance(data, np.ndarray):
            print(f"Skipping layer '{layer}': {split_key} data is not a numpy array (type: {type(data)})")
            continue
        
        if data.dtype == object:
            print(f"Skipping layer '{layer}': {split_key} data has object dtype")
            continue
        
        if data.ndim < 2:
            print(f"Skipping layer '{layer}': {split_key} data is not 2D (shape: {data.shape})")
            continue
        
        print(f"Using '{layer}' {split_key} data with shape {data.shape}")
        valid_activations[layer] = data
    
    # If no valid activations, try to use output
    if not valid_activations and 'output' in activations and isinstance(activations['output'], dict):
        split_key = 'test' if use_test_set else 'train'
        if split_key in activations['output']:
            data = activations['output'][split_key]
            
            # Convert list to numpy array if needed
            if isinstance(data, list):
                print(f"Converting output {split_key} list to numpy array...")
                try:
                    data = np.array(data)
                    print(f"Successfully converted to shape {data.shape}")
                except Exception as e:
                    print(f"Failed to convert output list to array: {e}")
                    data = None
            
            if isinstance(data, np.ndarray) and data.ndim >= 2 and data.dtype != object:
                print(f"No layer data found, using 'output' {split_key} as fallback (shape: {data.shape})")
                valid_activations['output'] = data
    
    if not valid_activations:
        raise ValueError(f"No valid layer activations found for {dataset}, {config}, seed {seed}")
    
    # Embed each layer
    embeddings = {}
    for layer, layer_activations in valid_activations.items():
        # Double-check before passing to fit_transform
        assert isinstance(layer_activations, np.ndarray), f"Layer {layer} is not ndarray: {type(layer_activations)}"
        assert layer_activations.dtype != object, f"Layer {layer} has object dtype"
        assert layer_activations.ndim >= 2, f"Layer {layer} is not 2D: shape={layer_activations.shape}"
        
        print(f"Embedding layer {layer}...")
        embeddings[layer] = embedder.fit_transform(layer_activations)
    
    return embeddings

def embed_all_configs(dataset: str, seeds: List[int] = [0, 1, 2], use_test_set: bool = True) -> Dict[str, Dict[str, Dict[str, np.ndarray]]]:
    """
    Embed all configurations (baseline and best) for the given dataset.
    
    Args:
        dataset: Name of the dataset.
        seeds: List of random seeds.
        use_test_set: Whether to use the test set (True) or train set (False)
        
    Returns:
        Nested dictionary mapping: config_name -> seed -> layer -> embedding.
    """
    # Get configurations
    baseline_config = get_baseline_config(dataset)
    best_config = get_best_config(dataset)
    
    # Create embedder
    embedder = Embedder()
    
    # Embed all configurations
    all_embeddings = {}
    
    # Baseline
    baseline_embeddings = {}
    for seed in seeds:
        print(f"Embedding baseline configuration for seed {seed}...")
        baseline_embeddings[seed] = embed_layer_activations(
            dataset, baseline_config, seed, embedder=embedder, use_test_set=use_test_set)
    all_embeddings["baseline"] = baseline_embeddings
    
    # Best config
    best_embeddings = {}
    for seed in seeds:
        print(f"Embedding best configuration for seed {seed}...")
        best_embeddings[seed] = embed_layer_activations(
            dataset, best_config, seed, embedder=embedder, use_test_set=use_test_set)
    all_embeddings["best"] = best_embeddings
    
    return all_embeddings

def main():
    """Main function for command-line usage."""
    parser = argparse.ArgumentParser(description="Embed layer activations for visualization.")
    parser.add_argument("datasets", nargs="+", help="Datasets to process (e.g., titanic, heart)")
    parser.add_argument("--seeds", type=int, nargs="+", default=[0, 1, 2], help="Seeds to process")
    parser.add_argument("--cache-dir", type=str, help="Cache directory for embeddings")
    
    args = parser.parse_args()
    
    # Process each dataset
    for dataset in args.datasets:
        try:
            print(f"Processing dataset: {dataset}")
            
            # Create embedder with specified cache directory
            embedder_kwargs = {}
            if args.cache_dir is not None:
                embedder_kwargs["cache_dir"] = args.cache_dir
            
            # Embed all configurations
            embed_all_configs(dataset, seeds=args.seeds)
            
        except Exception as e:
            print(f"Error processing dataset {dataset}: {e}")

if __name__ == "__main__":
    main() 