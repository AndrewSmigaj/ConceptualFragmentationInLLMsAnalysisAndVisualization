# Concept Fragmentation Architecture

**Last Updated**: 2025-05-30  
**Purpose**: Single source of truth for project organization. Review this at the start of every session.

## ğŸ”§ Environment Setup
**IMPORTANT**: This project uses a Python virtual environment located at `venv311/`
- Always activate the venv before running code: `source venv311/Scripts/activate` (Windows) or `source venv311/bin/activate` (Linux/Mac)
- Python version: 3.11
- All dependencies are installed in this venv

## ğŸ¯ Core Principles
1. **One implementation per feature** - No duplicates
2. **Clear ownership** - Every file has a clear purpose
3. **Hierarchical organization** - Components build on each other
4. **Self-documenting structure** - Directory names explain purpose

## ğŸ“ Directory Structure

```
ConceptualFragmentationInLLMsAnalysisAndVisualization/
â”‚
â”œâ”€â”€ ğŸ“¦ concept_fragmentation/          # CORE LIBRARY (New Architecture)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ activation/                    # Activation extraction & processing
â”‚   â”‚   â”œâ”€â”€ collector.py              # Collects activations from models
â”‚   â”‚   â”œâ”€â”€ processor.py              # Processes raw activations
â”‚   â”‚   â””â”€â”€ storage.py                # Manages activation storage
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                      # Analysis algorithms
â”‚   â”‚   â”œâ”€â”€ cross_layer_metrics.py    # Cross-layer fragmentation metrics
â”‚   â”‚   â”œâ”€â”€ similarity_metrics.py      # Similarity calculations
â”‚   â”‚   â”œâ”€â”€ token_analysis.py         # Token-level analysis
â”‚   â”‚   â””â”€â”€ transformer_metrics.py    # Transformer-specific metrics
â”‚   â”‚
â”‚   â”œâ”€â”€ clustering/                    # Clustering algorithms (Phase 1 âœ“)
â”‚   â”‚   â”œâ”€â”€ base.py                   # BaseClusterer abstract class
â”‚   â”‚   â”œâ”€â”€ paths.py                  # PathExtractor implementation
â”‚   â”‚   â””â”€â”€ exceptions.py             # Clustering exceptions
â”‚   â”‚
â”‚   â”œâ”€â”€ labeling/                      # Labeling strategies (Phase 1 âœ“)
â”‚   â”‚   â”œâ”€â”€ base.py                   # BaseLabeler abstract class
â”‚   â”‚   â””â”€â”€ exceptions.py             # Labeling exceptions
â”‚   â”‚
â”‚   â”œâ”€â”€ visualization/                 # Visualization components (Phase 2-3 âœ“)
â”‚   â”‚   â”œâ”€â”€ base.py                   # BaseVisualizer abstract class
â”‚   â”‚   â”œâ”€â”€ configs.py                # Configuration dataclasses
â”‚   â”‚   â”œâ”€â”€ sankey.py                 # Unified SankeyGenerator (Phase 2 âœ“)
â”‚   â”‚   â”œâ”€â”€ trajectory.py             # Unified TrajectoryVisualizer (Phase 3 âœ“)
â”‚   â”‚   â””â”€â”€ exceptions.py             # Visualization exceptions
â”‚   â”‚
â”‚   â”œâ”€â”€ experiments/                   # Experiment framework (Phase 1 âœ“)
â”‚   â”‚   â”œâ”€â”€ base.py                   # BaseExperiment abstract class
â”‚   â”‚   â””â”€â”€ config.py                 # ExperimentConfig management
â”‚   â”‚
â”‚   â”œâ”€â”€ persistence/                   # State management (Phase 1 âœ“)
â”‚   â”‚   â”œâ”€â”€ state.py                  # ExperimentState checkpointing
â”‚   â”‚   â””â”€â”€ gpt2_persistence.py       # GPT-2 specific persistence
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                          # LLM integration
â”‚   â”‚   â”œâ”€â”€ client.py                 # Base LLM client
â”‚   â”‚   â”œâ”€â”€ factory.py                # LLM provider factory
â”‚   â”‚   â””â”€â”€ analysis.py               # LLM-powered analysis
â”‚   â”‚
â”‚   â””â”€â”€ utils/                        # Utilities (Phase 1 âœ“)
â”‚       â”œâ”€â”€ logging.py                # Logging configuration
â”‚       â”œâ”€â”€ validation.py             # Data validation
â”‚       â””â”€â”€ path_utils.py             # Path manipulation
â”‚
â”œâ”€â”€ ğŸŒ visualization/                  # INTERACTIVE DASHBOARD
â”‚   â”œâ”€â”€ dash_app.py                   # Main dashboard application
â”‚   â”œâ”€â”€ run_dashboard.py              # Dashboard launcher
â”‚   â”œâ”€â”€ data_interface.py             # Data loading interface
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“Š Dashboard Tabs:
â”‚   â”œâ”€â”€ llm_tab.py                    # LLM analysis integration
â”‚   â”œâ”€â”€ gpt2_token_tab.py             # GPT-2 token analysis
â”‚   â”œâ”€â”€ path_metrics_tab.py           # Path metrics visualization
â”‚   â”œâ”€â”€ similarity_network_tab.py     # Network visualizations
â”‚   â”œâ”€â”€ path_fragmentation_tab.py     # Fragmentation analysis
â”‚   â”œâ”€â”€ cross_layer_viz.py            # Cross-layer visualizations
â”‚   â””â”€â”€ traj_plot.py                  # Trajectory plotting
â”‚
â”œâ”€â”€ ğŸ§ª experiments/                    # EXPERIMENT IMPLEMENTATIONS
â”‚   â”œâ”€â”€ gpt2/
â”‚   â”‚   â”œâ”€â”€ semantic_subtypes/        # Main GPT-2 case study
â”‚   â”‚   â”‚   â”œâ”€â”€ ğŸ¯ gpt2_semantic_subtypes_experiment.py  # MAIN ENTRY POINT
â”‚   â”‚   â”‚   â”œâ”€â”€ data/                 # Curated word lists (1,228 words)
â”‚   â”‚   â”‚   â”œâ”€â”€ *.json               # Configuration files
â”‚   â”‚   â”‚   â””â”€â”€ results/              # Experiment outputs
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ shared/                   # Shared GPT-2 utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ gpt2_activation_extractor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ gpt2_apa_metrics.py
â”‚   â”‚   â”‚   â””â”€â”€ gpt2_clustering_comparison.py
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ all_tokens/              # âš ï¸ CLEANUP CANDIDATE (2.5GB)
â”‚   â”‚
â”‚   â”œâ”€â”€ heart_disease/                # Heart disease case study
â”‚   â””â”€â”€ titanic/                      # Titanic case study
â”‚
â”œâ”€â”€ ğŸ“„ arxiv_submission/               # PAPER & FIGURES
â”‚   â”œâ”€â”€ main.tex                      # Paper source
â”‚   â”œâ”€â”€ sections/                     # Paper sections
â”‚   â”œâ”€â”€ figures/                      # Generated figures
â”‚   â””â”€â”€ references.bib                # Bibliography
â”‚
â”œâ”€â”€ ğŸ“š docs/                          # DOCUMENTATION
â”‚   â”œâ”€â”€ gpt2_analysis_guide.md        # GPT-2 analysis guide
â”‚   â”œâ”€â”€ llm_integration_guide.md      # LLM integration guide
â”‚   â””â”€â”€ visualization_plan.md         # Visualization roadmap
â”‚
â”œâ”€â”€ ğŸ§° scripts/                       # UTILITY SCRIPTS
â”‚   â”œâ”€â”€ analysis/                     # Analysis runners
â”‚   â”œâ”€â”€ maintenance/                  # Cleanup & maintenance
â”‚   â””â”€â”€ utilities/                    # Helper scripts
â”‚       â””â”€â”€ migrate_sankey_usage.py   # Migration tool (Phase 2)
â”‚
â”œâ”€â”€ ğŸ—ƒï¸ archive/                       # ARCHIVED CODE
â”‚   â””â”€â”€ (old implementations)         # Moved here during cleanup
â”‚
â”œâ”€â”€ ğŸ“‚ data/                          # CENTRALIZED DATA (New)
â”‚   â”œâ”€â”€ raw/                          # Original datasets
â”‚   â”œâ”€â”€ processed/                    # Preprocessed data
â”‚   â””â”€â”€ activations/                  # Neural network activations
â”‚
â”œâ”€â”€ ğŸ“Š results/                       # EXPERIMENT RESULTS (New)
â”‚   â”œâ”€â”€ gpt2/                         # GPT-2 experiments
â”‚   â”œâ”€â”€ heart_disease/                # Medical AI results
â”‚   â””â”€â”€ titanic/                      # Classic ML results
â”‚
â”œâ”€â”€ âš™ï¸ configs/                       # CONFIGURATION FILES (New)
â”‚   â”œâ”€â”€ experiments/                  # Experiment configs
â”‚   â”œâ”€â”€ models/                       # Model configs
â”‚   â””â”€â”€ visualization/                # Viz settings
â”‚
â”œâ”€â”€ ğŸ§ª tests/                         # CONSOLIDATED TESTS (New)
â”‚   â”œâ”€â”€ unit/                         # Unit tests
â”‚   â”œâ”€â”€ integration/                  # Integration tests
â”‚   â””â”€â”€ legacy/                       # Old test files
â”‚
â””â”€â”€ ğŸ“¦ Key Entry Points:
    â”œâ”€â”€ ğŸš€ python visualization/run_dashboard.py          # Launch dashboard
    â”œâ”€â”€ ğŸ§ª python experiments/gpt2/semantic_subtypes/gpt2_semantic_subtypes_experiment.py
    â””â”€â”€ ğŸ“Š python -m concept_fragmentation.analysis.cross_layer_metrics
```

## ğŸ”„ Refactoring Status

### âœ… Completed Phases
1. **Phase 1**: Core directory structure with base classes
2. **Phase 2**: Unified SankeyGenerator consolidation
3. **Phase 3**: Unified TrajectoryVisualizer (completed)
4. **Phase 4**: Repository reorganization (in progress)
   - Created centralized data/, results/, configs/, tests/ directories
   - Updated documentation for CTA focus
   - Structure designed to support future bigram experiments

### ğŸš§ Upcoming Phases
5. **Phase 5**: Consolidate GPT-2 analysis scripts
6. **Phase 6**: Unify dashboard components
7. **Phase 7**: Clean up experiments/gpt2/all_tokens/
8. **Phase 8**: Implement bigram experiment framework

## ğŸ“‹ File Discipline Rules

### âœ… DO
- Check this architecture before creating ANY new file
- Use existing base classes and utilities
- Put new visualizations in concept_fragmentation/visualization/
- Create tests for new components
- Update this document when adding major components

### âŒ DON'T
- Create "test_quick.py" or "analyze_fast.py" files
- Duplicate existing functionality
- Put visualization code in experiments/
- Create multiple versions (basic_, improved_, enhanced_)
- Leave temporary files in the repo

## ğŸ¯ When You Need To...

### Create a New Visualization
1. Check if concept_fragmentation/visualization/ has something similar
2. Extend BaseVisualizer
3. Add configuration to configs.py
4. Create tests in visualization/tests/

### Add New Analysis
1. Check concept_fragmentation/analysis/ first
2. Consider if it belongs in an existing module
3. Follow the pattern of existing analyzers

### Run an Experiment
1. Use concept_fragmentation/experiments/base.py
2. Store configs in experiments/[dataset]/config/
3. Save results in experiments/[dataset]/results/

### Add LLM Features
1. Use concept_fragmentation/llm/factory.py
2. Don't create new API clients
3. Follow existing patterns in llm/analysis.py

## ğŸš¨ Cleanup Needed

### High Priority (Blocking Progress)
- [ ] experiments/gpt2/all_tokens/ - 2.5GB of duplicates
- [ ] Multiple sankey implementations â†’ Use unified SankeyGenerator
- [ ] Duplicate label creation scripts

### Medium Priority (Confusing)
- [ ] Multiple dashboard utility versions
- [ ] Test scripts scattered everywhere
- [ ] Old visualization implementations

### Low Priority (Nice to Have)
- [ ] Compress old results
- [ ] Archive completed experiments
- [ ] Clean up logs

## ğŸ“ Session Checklist

At the start of each session:
1. [ ] Review this ARCHITECTURE.md
2. [ ] Check REFACTOR_LOG.md for recent changes
3. [ ] Run key entry points to ensure they work
4. [ ] Update todo list based on priorities

## ğŸ” Quick Reference

**Need to visualize trajectories?**
â†’ `from concept_fragmentation.visualization.trajectory import TrajectoryVisualizer`

**Need to create Sankey diagrams?**
â†’ `from concept_fragmentation.visualization.sankey import SankeyGenerator`

**Need to analyze cross-layer metrics?**
â†’ `from concept_fragmentation.analysis.cross_layer_metrics import ...`

**Need to run GPT-2 experiment?**
â†’ `python experiments/gpt2/semantic_subtypes/gpt2_semantic_subtypes_experiment.py`

**Need to launch dashboard?**
â†’ `python visualization/run_dashboard.py`

---

**Remember**: Every file should have ONE clear purpose and ONE clear location. When in doubt, ask: "Where would I look for this in 6 months?"