# Implementation Plan: Concept Fragmentation in Neural Networks â€“ Visualizing and Measuring Intra-Class Dispersion in Feedforward Models

## 1. Overview
This document describes the engineering roadmap for the paper *"Concept Fragmentation in Neural Networks: Visualizing and Measuring Intra-Class Dispersion in Feedforward Models."*  The goal is to build an **open-source, fully reproducible** toolkit that (a) trains baseline and regularized feed-forward models, (b) captures layer activations, (c) computes fragmentation metrics, and (d) produces publication-ready visualisations.

Key deliverables:
1. Metric implementations for **Cluster Entropy** and **Subspace Angle**
2. Activation tracing utilities that scale from tiny (3Ã—3Ã—3) to wide/deep networks
3. Cohesion regularisation (contrastive loss) plugin
4. Scripts for end-to-end experiments on four datasets (Titanic, Adult Income, Heart Disease, Fashion-MNIST subset)
5. Figures & tables for the final paper + interactive notebooks

---

## 2. Current Repository Status (2025-05-13)
âœ… Implemented
* `concept_fragmentation/models/feedforward.py` â€“ 3-layer MLP with named layers & activation cache
* `concept_fragmentation/models/regularizers.py` â€“ Cohesion regulariser (InfoNCE-style)
* `concept_fragmentation/data/` â€“ dataset loaders (`loaders.py`) and preprocessing utilities (`preprocessors.py`)

ðŸš§ **Missing / Empty Stubs**
* Activation hooks (`concept_fragmentation/hooks/activation_hooks.py`)
* Fragmentation metrics (`metrics/cluster_entropy.py`, `metrics/subspace_angle.py`)
* Visualisation utilities (`visualization/activations.py`, `visualization/trajectories.py`)
* Experiment scripts (`experiments/train.py`, `experiments/evaluate.py`, `experiments/visualize.py`)
* Utility helpers (`utils/helpers.py`)
* Tests (`tests/`)
* `config.py`, `requirements.txt`, and repo-level `README.md`

Action items below focus on filling these gaps.

---

## 3. Repository Layout
```
concept_fragmentation/
â”œâ”€â”€ config.py                  # All hyper-parameters & dataset paths
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ loaders.py             # Titanic, Adult, Heart, Fashion-MNIST
â”‚   â””â”€â”€ preprocessors.py       # One-hot, scaling, imputation
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ feedforward.py         # Baseline MLP (done)
â”‚   â””â”€â”€ regularizers.py        # Cohesion regulariser (done)
â”œâ”€â”€ hooks/
â”‚   â””â”€â”€ activation_hooks.py    # Forward hooks for activation capture (TODO)
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ cluster_entropy.py     # Intra-class k-means entropy (TODO)
â”‚   â””â”€â”€ subspace_angle.py      # Pairwise principal angles (TODO)
â”œâ”€â”€ visualization/
â”‚   â”œâ”€â”€ activations.py         # PCA/UMAP scatter & layer overlays (TODO)
â”‚   â””â”€â”€ trajectories.py        # Per-sample activation paths (TODO)
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train.py               # Training loop + logging (TODO)
â”‚   â”œâ”€â”€ evaluate.py            # Metric computation (TODO)
â”‚   â””â”€â”€ visualize.py           # Figure generation (TODO)
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ helpers.py             # Seed fixing, logging, CLI wrappers (TODO)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_metrics.py        # Unit tests for metrics (TODO)
â”‚   â””â”€â”€ test_hooks.py          # Hook integrity tests (TODO)
â””â”€â”€ notebooks/                 # Exploratory analysis & paper figures
```

---

## 4. Detailed Task Breakdown
### 4.1 Hooks Module â€“ Activation Capture (Priority-High)
* Implement generic forward hook registration that stores activations in a memory-efficient dict keyed by layer name
* Add **top-k neuron selection** option for sparse visualisation (default *k* = 3)
* Provide context-manager wrapper so hooks are auto-removed after use

### 4.2 Metrics Module (Priority-High)
1. **Cluster Entropy**
   * Fit *k*-means **once** on the entire activation matrix of a layer.
   * **Automatic choice of K**: we evaluate the silhouette score for K = 2 â€¦ K_max (K_max = min(12, âˆšN)) and pick the K that maximises the score.
   * For each class *c*, compute the proportion pâ‚–(c) of its samples that fall into each selected cluster k.
   * Compute entropy  H_c = âˆ’Î£â‚– pâ‚–(c) logâ‚‚ pâ‚–(c) and normalise by logâ‚‚ K.
   * Return per-class and aggregate (mean / max) scores
2. **Subspace Angle**
   * Take 90 % variance principal components per class
   * Bootstrap class activations (B=10), compute pairwise principal angles Î¸
   * Aggregate statistics: mean Î¸, std, 95 % CI

### 4.3 Visualisation Module
* `activations.py` â€“ 2D/3D PCA & UMAP scatter, coloured by class & sub-cluster
* `trajectories.py` â€“ line plots of per-sample activation over layers (tiny nets) and PCA-compressed paths (larger nets)
* All plots seeded for determinism, matplotlib + seaborn styling

### 4.4 Experiment Scripts
* `train.py`
  * CLI: dataset, model size, regularisation flag, seeds
  * Logs: train/val loss, accuracy, fragmentation metrics per epoch
* `evaluate.py`
  * Loads checkpoint, computes metrics on test split
* `visualize.py`
  * Generates & saves figures used in paper

### 4.5 Testing & CI
* PyTest suites covering metrics correctness, hook output shapes, reproducibility
* GitHub Actions workflow: lint â†’ unit tests â†’ notebook smoke test

### 4.6 Documentation & Config
* Populate `config.py` with default hyper-parameters and dataset paths
* Fill `requirements.txt` with pinned versions (see Section 7)
* Write a thorough `README.md` with quick-start instructions

---

## 5. Cohesion Regularisation Hyper-Parameter Grid
| Weight Î» | Temperature Ï„ | Similarity Threshold | Layers              |
|---------:|--------------:|----------------------|---------------------|
| 0 (baseline) | 0.07 | 0.0 | â€“ |
| 0.1 | 0.07 | 0.0 | layer3 |
| 0.1 | 0.07 | 0.3 | layer3 |
| 0.5 | 0.07 | 0.3 | layer2+layer3 |

---

## 6. Timeline (â‰ˆ10 Weeks)
1. **Week 1â€“2 Foundation** â€“ Finish hooks, metrics, config, and basic tests
2. **Week 3â€“4 Experiments v0** â€“ Implement training script, run tiny nets
3. **Week 5â€“6 Regularisation & Visuals** â€“ Integrate cohesion loss; implement visualisation utilities
4. **Week 7â€“8 Full Experiments** â€“ All datasets, hyper-parameter sweeps, statistical tests
5. **Week 9â€“10 Paper Prep** â€“ Final figures, notebooks, documentation

---

## 7. Dependencies (`requirements.txt`)
```
numpy>=1.20.0
pandas>=1.3.0
scikit-learn>=1.0.0
torch>=1.10.0
matplotlib>=3.4.0
seaborn>=0.11.0
umap-learn>=0.5.0
jupyter>=1.0.0
pytest>=6.0.0
```

---

## 8. Next Steps
1. **Metrics module** â€“ implement and unit-test (`cluster_entropy.py`, `subspace_angle.py`)
2. **Activation hooks** â€“ implement `activation_hooks.py`, integrate with `FeedforwardNetwork`
3. **Populate config & requirements** â€“ essential for running anything
4. **Discuss**: Do we want to support non-tabular datasets beyond the current list in this release? 