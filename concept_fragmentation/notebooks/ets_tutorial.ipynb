{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainable Threshold Similarity (ETS) Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the Explainable Threshold Similarity (ETS) clustering algorithm for transparent, dimension-wise cluster analysis.\n",
    "\n",
    "## What is ETS?\n",
    "\n",
    "Explainable Threshold Similarity (ETS) is a clustering approach described in the paper \"Foundations of Archetypal Path Analysis: Toward a Principled Geometry for Cluster-Based Interpretability\". It provides several advantages over traditional clustering methods:\n",
    "\n",
    "1. **Dimension-wise interpretability**: Each dimension has its own threshold that determines similarity\n",
    "2. **Explicit bounds on membership**: Clear criteria for when points belong to the same cluster\n",
    "3. **Compatibility with heterogeneous feature scales**: Works well with dimensions of vastly different scales\n",
    "4. **Transparent explanations**: Can explain exactly why two points are in the same or different clusters\n",
    "\n",
    "ETS declares two activations similar if $|a^l_{i,j} - a^l_{k,j}| \\leq \\tau_j$ for every dimension $j$. Cluster membership can therefore be verbalized as \"neuron j differs by less than $\\tau_j$.\"\n",
    "\n",
    "Let's begin by importing the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import ETS functions\n",
    "from concept_fragmentation.metrics.explainable_threshold_similarity import (\n",
    "    compute_ets_clustering,\n",
    "    compute_dimension_thresholds,\n",
    "    explain_ets_similarity,\n",
    "    compute_ets_statistics\n",
    ")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic ETS Clustering Example\n",
    "\n",
    "Let's start with a simple 2D example to visualize how ETS clustering works. We'll create a synthetic dataset with clear clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data with 3 clusters\n",
    "def generate_synthetic_data(n_samples=300, n_clusters=3, n_features=2, cluster_std=0.5):\n",
    "    # Generate cluster centers\n",
    "    centers = np.random.uniform(-10, 10, (n_clusters, n_features))\n",
    "    \n",
    "    # Ensure centers are well-separated\n",
    "    for i in range(n_clusters):\n",
    "        for j in range(i+1, n_clusters):\n",
    "            while np.linalg.norm(centers[i] - centers[j]) < 7.0:\n",
    "                centers[j] = np.random.uniform(-10, 10, n_features)\n",
    "    \n",
    "    # Generate data points\n",
    "    X = np.zeros((n_samples, n_features))\n",
    "    y = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    samples_per_cluster = n_samples // n_clusters\n",
    "    for i in range(n_clusters):\n",
    "        start_idx = i * samples_per_cluster\n",
    "        end_idx = (i + 1) * samples_per_cluster if i < n_clusters - 1 else n_samples\n",
    "        \n",
    "        # Generate points around cluster center\n",
    "        X[start_idx:end_idx] = centers[i] + np.random.normal(0, cluster_std, (end_idx - start_idx, n_features))\n",
    "        y[start_idx:end_idx] = i\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y_true = generate_synthetic_data(n_samples=300, n_clusters=3, n_features=2, cluster_std=0.5)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_true, cmap='viridis', alpha=0.8, s=50)\n",
    "plt.title('Synthetic Dataset with 3 Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='True Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Running ETS Clustering\n",
    "\n",
    "Now, let's apply ETS clustering to this dataset and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ETS clustering\n",
    "threshold_percentile = 0.1  # Use 10th percentile of pairwise differences as threshold\n",
    "ets_labels, thresholds = compute_ets_clustering(\n",
    "    X,\n",
    "    threshold_percentile=threshold_percentile,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Calculate number of clusters found\n",
    "n_clusters_ets = len(np.unique(ets_labels))\n",
    "print(f\"Number of clusters found by ETS: {n_clusters_ets}\")\n",
    "print(f\"Computed thresholds: {thresholds}\")\n",
    "\n",
    "# Plot ETS clustering results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=ets_labels, cmap='viridis', alpha=0.8, s=50)\n",
    "plt.title(f'ETS Clustering Results ({n_clusters_ets} clusters)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Comparing with K-means\n",
    "\n",
    "Let's compare the ETS results with traditional k-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Plot k-means clustering results\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='viridis', alpha=0.8, s=50)\n",
    "plt.title('K-means Clustering Results (3 clusters)')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Compare clustering metrics\n",
    "if n_clusters_ets > 1 and len(np.unique(kmeans_labels)) > 1:\n",
    "    silhouette_ets = silhouette_score(X, ets_labels)\n",
    "    silhouette_kmeans = silhouette_score(X, kmeans_labels)\n",
    "    \n",
    "    ari_ets = adjusted_rand_score(y_true, ets_labels)\n",
    "    ari_kmeans = adjusted_rand_score(y_true, kmeans_labels)\n",
    "    \n",
    "    print(f\"ETS Silhouette Score: {silhouette_ets:.3f}\")\n",
    "    print(f\"K-means Silhouette Score: {silhouette_kmeans:.3f}\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"ETS Adjusted Rand Index (vs true labels): {ari_ets:.3f}\")\n",
    "    print(f\"K-means Adjusted Rand Index (vs true labels): {ari_kmeans:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding ETS Thresholds\n",
    "\n",
    "One of the key features of ETS is its dimension-specific thresholds. Let's explore how these thresholds work and how they affect clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the thresholds\n",
    "feature_names = [f'Feature {i+1}' for i in range(X.shape[1])]\n",
    "\n",
    "# Calculate data ranges for context\n",
    "data_min = np.min(X, axis=0)\n",
    "data_max = np.max(X, axis=0)\n",
    "data_range = data_max - data_min\n",
    "\n",
    "# Calculate relative thresholds (as percentage of data range)\n",
    "relative_thresholds = thresholds / data_range * 100\n",
    "\n",
    "# Create a DataFrame for easier plotting\n",
    "threshold_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Absolute Threshold': thresholds,\n",
    "    'Relative Threshold (%)': relative_thresholds,\n",
    "    'Data Range': data_range\n",
    "})\n",
    "\n",
    "print(\"Dimension-wise Thresholds:\")\n",
    "display(threshold_df)\n",
    "\n",
    "# Plot threshold values\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Absolute thresholds\n",
    "ax[0].bar(feature_names, thresholds)\n",
    "ax[0].set_title('Absolute Thresholds per Dimension')\n",
    "ax[0].set_ylabel('Threshold Value')\n",
    "ax[0].set_xlabel('Dimension')\n",
    "\n",
    "# Relative thresholds\n",
    "ax[1].bar(feature_names, relative_thresholds)\n",
    "ax[1].set_title('Relative Thresholds (% of Dimension Range)')\n",
    "ax[1].set_ylabel('Threshold (% of Range)')\n",
    "ax[1].set_xlabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Threshold Selection Impact\n",
    "\n",
    "Let's explore how different threshold percentiles affect the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different threshold percentiles\n",
    "percentiles = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5]\n",
    "results = []\n",
    "\n",
    "for percentile in percentiles:\n",
    "    # Compute ETS clustering\n",
    "    labels, thresholds = compute_ets_clustering(X, threshold_percentile=percentile)\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    \n",
    "    # Compute metrics if possible\n",
    "    if n_clusters > 1:\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "        ari = adjusted_rand_score(y_true, labels)\n",
    "    else:\n",
    "        silhouette = 0.0\n",
    "        ari = 0.0\n",
    "    \n",
    "    # Store results\n",
    "    results.append({\n",
    "        'percentile': percentile,\n",
    "        'n_clusters': n_clusters,\n",
    "        'silhouette': silhouette,\n",
    "        'ari': ari,\n",
    "        'thresholds': thresholds\n",
    "    })\n",
    "\n",
    "# Plot results\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Number of clusters vs percentile\n",
    "axs[0, 0].plot([r['percentile'] for r in results], [r['n_clusters'] for r in results], 'o-')\n",
    "axs[0, 0].set_title('Number of Clusters vs Threshold Percentile')\n",
    "axs[0, 0].set_xlabel('Threshold Percentile')\n",
    "axs[0, 0].set_ylabel('Number of Clusters')\n",
    "\n",
    "# Silhouette score vs percentile\n",
    "axs[0, 1].plot([r['percentile'] for r in results], [r['silhouette'] for r in results], 'o-')\n",
    "axs[0, 1].set_title('Silhouette Score vs Threshold Percentile')\n",
    "axs[0, 1].set_xlabel('Threshold Percentile')\n",
    "axs[0, 1].set_ylabel('Silhouette Score')\n",
    "\n",
    "# ARI vs percentile\n",
    "axs[1, 0].plot([r['percentile'] for r in results], [r['ari'] for r in results], 'o-')\n",
    "axs[1, 0].set_title('Adjusted Rand Index vs Threshold Percentile')\n",
    "axs[1, 0].set_xlabel('Threshold Percentile')\n",
    "axs[1, 0].set_ylabel('Adjusted Rand Index')\n",
    "\n",
    "# Thresholds vs percentile\n",
    "thresholds_0 = [r['thresholds'][0] for r in results]\n",
    "thresholds_1 = [r['thresholds'][1] for r in results]\n",
    "axs[1, 1].plot([r['percentile'] for r in results], thresholds_0, 'o-', label='Feature 1')\n",
    "axs[1, 1].plot([r['percentile'] for r in results], thresholds_1, 'o-', label='Feature 2')\n",
    "axs[1, 1].set_title('Threshold Values vs Threshold Percentile')\n",
    "axs[1, 1].set_xlabel('Threshold Percentile')\n",
    "axs[1, 1].set_ylabel('Threshold Value')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal threshold percentile\n",
    "best_percentile = max(results, key=lambda x: x['ari'])['percentile']\n",
    "print(f\"Optimal threshold percentile based on ARI: {best_percentile}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ETS Explanation Capability\n",
    "\n",
    "One of the most powerful features of ETS is its ability to explain *why* two points are in the same or different clusters. Let's explore this functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the clustering results from the optimal threshold percentile\n",
    "optimal_ets = next(r for r in results if r['percentile'] == best_percentile)\n",
    "optimal_labels = compute_ets_clustering(X, threshold_percentile=best_percentile)[0]\n",
    "optimal_thresholds = optimal_ets['thresholds']\n",
    "\n",
    "# Find pairs of points from same and different clusters\n",
    "cluster_ids = np.unique(optimal_labels)\n",
    "\n",
    "# Same cluster pair\n",
    "cluster_id = cluster_ids[0]\n",
    "points_in_cluster = np.where(optimal_labels == cluster_id)[0]\n",
    "same_pair = (points_in_cluster[0], points_in_cluster[1])\n",
    "\n",
    "# Different cluster pair\n",
    "cluster_id1 = cluster_ids[0]\n",
    "cluster_id2 = cluster_ids[1] if len(cluster_ids) > 1 else cluster_ids[0]\n",
    "points_in_cluster1 = np.where(optimal_labels == cluster_id1)[0]\n",
    "points_in_cluster2 = np.where(optimal_labels == cluster_id2)[0]\n",
    "diff_pair = (points_in_cluster1[0], points_in_cluster2[0] if cluster_id1 != cluster_id2 else points_in_cluster1[2])\n",
    "\n",
    "# Generate explanations\n",
    "same_explanation = explain_ets_similarity(\n",
    "    X[same_pair[0]], \n",
    "    X[same_pair[1]], \n",
    "    optimal_thresholds,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "diff_explanation = explain_ets_similarity(\n",
    "    X[diff_pair[0]], \n",
    "    X[diff_pair[1]], \n",
    "    optimal_thresholds,\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "# Display explanations\n",
    "print(\"Points in Same Cluster:\")\n",
    "print(f\"Point 1: {X[same_pair[0]]}\")\n",
    "print(f\"Point 2: {X[same_pair[1]]}\")\n",
    "print(f\"Explanation: {same_explanation['is_similar'] and 'Similar' or 'Different'}\")\n",
    "print(f\"Dimensions within threshold: {same_explanation['num_dimensions_within_threshold']}/{same_explanation['num_dimensions_compared']}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Points in Different Clusters:\")\n",
    "print(f\"Point 1: {X[diff_pair[0]]}\")\n",
    "print(f\"Point 2: {X[diff_pair[1]]}\")\n",
    "print(f\"Explanation: {diff_explanation['is_similar'] and 'Similar' or 'Different'}\")\n",
    "print(f\"Dimensions within threshold: {diff_explanation['num_dimensions_within_threshold']}/{diff_explanation['num_dimensions_compared']}\")\n",
    "print(f\"Distinguishing dimensions: {', '.join(diff_explanation['distinguishing_dimensions'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize explanation\n",
    "def visualize_explanation(point1, point2, thresholds, feature_names=None):\n",
    "    \"\"\"Visualize the explanation for why two points are similar or different.\"\"\"\n",
    "    n_features = len(point1)\n",
    "    \n",
    "    # Set feature names if not provided\n",
    "    if feature_names is None:\n",
    "        feature_names = [f\"Dimension {i}\" for i in range(n_features)]\n",
    "    \n",
    "    # Compute dimension-wise absolute differences\n",
    "    diffs = np.abs(point1 - point2)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Create indices for the bars\n",
    "    indices = np.arange(n_features)\n",
    "    \n",
    "    # Plot differences and thresholds\n",
    "    width = 0.35\n",
    "    ax.bar(indices - width/2, diffs, width, label=\"Absolute Difference\")\n",
    "    ax.bar(indices + width/2, thresholds, width, label=\"Threshold\")\n",
    "    \n",
    "    # Highlight dimensions that exceed threshold\n",
    "    for i in range(n_features):\n",
    "        if diffs[i] > thresholds[i]:\n",
    "            ax.axvspan(i - 0.5, i + 0.5, alpha=0.2, color='red')\n",
    "    \n",
    "    # Show whether points are similar\n",
    "    is_similar = np.all(diffs <= thresholds)\n",
    "    similarity_text = \"Points are in the same cluster\" if is_similar else \"Points are in different clusters\"\n",
    "    ax.set_title(f\"Dimension-wise Comparison: {similarity_text}\")\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xlabel(\"Dimensions\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.set_xticks(indices)\n",
    "    ax.set_xticklabels(feature_names)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add values annotation\n",
    "    for i in range(n_features):\n",
    "        ax.annotate(f\"{point1[i]:.2f}\",\n",
    "                  xy=(i - width/2, diffs[i] + 0.1),\n",
    "                  xytext=(0, 3),\n",
    "                  textcoords=\"offset points\",\n",
    "                  ha='center', va='bottom',\n",
    "                  fontsize=8)\n",
    "        ax.annotate(f\"{point2[i]:.2f}\",\n",
    "                  xy=(i - width/2, 0),\n",
    "                  xytext=(0, -15),\n",
    "                  textcoords=\"offset points\",\n",
    "                  ha='center', va='top',\n",
    "                  fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Visualize same-cluster explanation\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "visualize_explanation(X[same_pair[0]], X[same_pair[1]], optimal_thresholds, feature_names)\n",
    "plt.title(\"Explanation for Points in Same Cluster\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Visualize different-cluster explanation\n",
    "plt.subplot(2, 1, 2)\n",
    "visualize_explanation(X[diff_pair[0]], X[diff_pair[1]], optimal_thresholds, feature_names)\n",
    "plt.title(\"Explanation for Points in Different Clusters\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with Mixed-Scale Features\n",
    "\n",
    "One of the key advantages of ETS is its ability to handle features with very different scales. Let's create a synthetic dataset with mixed scales and see how ETS performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with mixed scales\n",
    "def generate_mixed_scale_data(n_samples=300, n_clusters=3):\n",
    "    # Feature scales with dramatically different ranges\n",
    "    feature_scales = [\n",
    "        1.0,       # Regular scale (0-1)\n",
    "        100.0,     # Large scale (0-100)\n",
    "        0.01,      # Small scale (0-0.01)\n",
    "        10.0,      # Medium scale (0-10)\n",
    "    ]\n",
    "    \n",
    "    # Generate data with these mixed scales\n",
    "    X, y = generate_synthetic_data(\n",
    "        n_samples=n_samples,\n",
    "        n_clusters=n_clusters,\n",
    "        n_features=len(feature_scales),\n",
    "        cluster_std=0.2  # Keep clusters tight for this example\n",
    "    )\n",
    "    \n",
    "    # Apply the scales\n",
    "    for j in range(X.shape[1]):\n",
    "        X[:, j] *= feature_scales[j]\n",
    "    \n",
    "    return X, y, feature_scales\n",
    "\n",
    "# Generate mixed-scale data\n",
    "X_mixed, y_mixed, feature_scales = generate_mixed_scale_data(n_samples=300, n_clusters=3)\n",
    "\n",
    "# Display scale information\n",
    "print(\"Feature Scales:\")\n",
    "for i, scale in enumerate(feature_scales):\n",
    "    print(f\"Feature {i+1}: {scale}\")\n",
    "\n",
    "# Show data ranges\n",
    "data_min = np.min(X_mixed, axis=0)\n",
    "data_max = np.max(X_mixed, axis=0)\n",
    "data_range = data_max - data_min\n",
    "\n",
    "print(\"\\nData Ranges:\")\n",
    "for i in range(X_mixed.shape[1]):\n",
    "    print(f\"Feature {i+1}: {data_min[i]:.3f} to {data_max[i]:.3f} (range: {data_range[i]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ETS clustering on mixed-scale data\n",
    "mixed_labels, mixed_thresholds = compute_ets_clustering(\n",
    "    X_mixed,\n",
    "    threshold_percentile=0.1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run k-means for comparison\n",
    "# K-means is sensitive to scale, so we'll also run it on normalized data\n",
    "kmeans_raw = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_raw_labels = kmeans_raw.fit_predict(X_mixed)\n",
    "\n",
    "# Normalize data for k-means\n",
    "scaler = StandardScaler()\n",
    "X_mixed_scaled = scaler.fit_transform(X_mixed)\n",
    "kmeans_scaled = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans_scaled_labels = kmeans_scaled.fit_predict(X_mixed_scaled)\n",
    "\n",
    "# Compare results\n",
    "ari_ets = adjusted_rand_score(y_mixed, mixed_labels)\n",
    "ari_kmeans_raw = adjusted_rand_score(y_mixed, kmeans_raw_labels)\n",
    "ari_kmeans_scaled = adjusted_rand_score(y_mixed, kmeans_scaled_labels)\n",
    "\n",
    "print(f\"ETS ARI vs true labels: {ari_ets:.3f}\")\n",
    "print(f\"K-means (raw data) ARI vs true labels: {ari_kmeans_raw:.3f}\")\n",
    "print(f\"K-means (scaled data) ARI vs true labels: {ari_kmeans_scaled:.3f}\")\n",
    "\n",
    "# Visualize thresholds relative to data scales\n",
    "mixed_feature_names = [f'Feature {i+1}' for i in range(X_mixed.shape[1])]\n",
    "\n",
    "# Calculate relative thresholds (as percentage of data range)\n",
    "relative_thresholds = mixed_thresholds / data_range * 100\n",
    "\n",
    "# Create a DataFrame for easier display\n",
    "threshold_scale_df = pd.DataFrame({\n",
    "    'Feature': mixed_feature_names,\n",
    "    'Feature Scale': feature_scales,\n",
    "    'Data Range': data_range,\n",
    "    'Absolute Threshold': mixed_thresholds,\n",
    "    'Relative Threshold (%)': relative_thresholds\n",
    "})\n",
    "\n",
    "print(\"\\nThresholds Relative to Feature Scales:\")\n",
    "display(threshold_scale_df)\n",
    "\n",
    "# Plot threshold adaptation\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Feature scale vs threshold\n",
    "axs[0].scatter(feature_scales, mixed_thresholds)\n",
    "axs[0].set_title('Threshold vs Feature Scale')\n",
    "axs[0].set_xlabel('Feature Scale')\n",
    "axs[0].set_ylabel('Threshold')\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_yscale('log')\n",
    "\n",
    "# Feature vs relative threshold\n",
    "axs[1].bar(mixed_feature_names, relative_thresholds)\n",
    "axs[1].set_title('Relative Threshold by Feature')\n",
    "axs[1].set_xlabel('Feature')\n",
    "axs[1].set_ylabel('Relative Threshold (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Real-World Example: Titanic Dataset\n",
    "\n",
    "Let's apply ETS to a real-world dataset - the Titanic passenger data. We'll use it to identify natural clusters of passengers and explain their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Select and preprocess features\n",
    "# We'll use a subset of features for simplicity\n",
    "features = ['age', 'fare', 'pclass', 'sex']\n",
    "\n",
    "# Handle missing values\n",
    "titanic_clean = titanic.copy()\n",
    "titanic_clean['age'].fillna(titanic_clean['age'].median(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "titanic_clean['sex'] = titanic_clean['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# Extract features and target\n",
    "X_titanic = titanic_clean[features].values\n",
    "y_titanic = titanic_clean['survived'].values\n",
    "\n",
    "# Show dataset info\n",
    "print(\"Titanic Dataset:\")\n",
    "print(f\"Number of samples: {X_titanic.shape[0]}\")\n",
    "print(f\"Number of features: {X_titanic.shape[1]}\")\n",
    "print(f\"Features: {features}\")\n",
    "\n",
    "# Display feature statistics\n",
    "titanic_stats = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Min': np.min(X_titanic, axis=0),\n",
    "    'Max': np.max(X_titanic, axis=0),\n",
    "    'Mean': np.mean(X_titanic, axis=0),\n",
    "    'Std': np.std(X_titanic, axis=0)\n",
    "})\n",
    "\n",
    "display(titanic_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run ETS clustering on Titanic data\n",
    "titanic_labels, titanic_thresholds = compute_ets_clustering(\n",
    "    X_titanic,\n",
    "    threshold_percentile=0.1,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate clustering\n",
    "n_clusters = len(np.unique(titanic_labels))\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "\n",
    "if n_clusters > 1:\n",
    "    silhouette = silhouette_score(X_titanic, titanic_labels)\n",
    "    print(f\"Silhouette score: {silhouette:.3f}\")\n",
    "\n",
    "# Compute cluster statistics\n",
    "stats = compute_ets_statistics(X_titanic, titanic_labels, titanic_thresholds)\n",
    "\n",
    "print(\"\\nETS Clustering Statistics:\")\n",
    "print(f\"Number of clusters: {stats['n_clusters']}\")\n",
    "print(f\"Cluster sizes (min/max/mean): {stats['cluster_sizes']['min']}/{stats['cluster_sizes']['max']}/{stats['cluster_sizes']['mean']:.1f}\")\n",
    "print(f\"Active dimensions per cluster (min/max/mean): {stats['active_dimensions']['min']}/{stats['active_dimensions']['max']}/{stats['active_dimensions']['mean']:.1f}\")\n",
    "\n",
    "# Sort dimensions by importance\n",
    "importance = sorted([(int(k), v) for k, v in stats['dimension_importance'].items()], key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nDimension Importance (which dimensions are most active in defining clusters):\")\n",
    "for dim, value in importance:\n",
    "    print(f\"{features[dim]}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze clusters in terms of original features and survival rate\n",
    "cluster_stats = []\n",
    "\n",
    "for cluster_id in range(stats['n_clusters']):\n",
    "    cluster_mask = (titanic_labels == cluster_id)\n",
    "    cluster_data = X_titanic[cluster_mask]\n",
    "    cluster_survival = y_titanic[cluster_mask]\n",
    "    \n",
    "    # Compute statistics\n",
    "    feature_means = np.mean(cluster_data, axis=0)\n",
    "    survival_rate = np.mean(cluster_survival)\n",
    "    count = np.sum(cluster_mask)\n",
    "    \n",
    "    # Store statistics\n",
    "    cluster_stats.append({\n",
    "        'cluster_id': cluster_id,\n",
    "        'count': count,\n",
    "        'percentage': count / len(titanic_labels) * 100,\n",
    "        'survival_rate': survival_rate * 100,\n",
    "        **{features[i]: feature_means[i] for i in range(len(features))}\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame\n",
    "cluster_df = pd.DataFrame(cluster_stats)\n",
    "\n",
    "# Decode sex back to string for clarity\n",
    "cluster_df['sex'] = cluster_df['sex'].map(lambda x: 'female' if x > 0.5 else 'male')\n",
    "\n",
    "print(\"Cluster Statistics:\")\n",
    "display(cluster_df)\n",
    "\n",
    "# Visualize clusters in PCA space, colored by cluster\n",
    "pca = PCA(n_components=2)\n",
    "X_titanic_pca = pca.fit_transform(X_titanic)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(X_titanic_pca[:, 0], X_titanic_pca[:, 1], c=titanic_labels, cmap='viridis', alpha=0.7, s=50)\n",
    "plt.title(f'Titanic Dataset: ETS Clusters ({n_clusters} clusters)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize survival rate by cluster\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(cluster_df['cluster_id'], cluster_df['survival_rate'])\n",
    "plt.title('Survival Rate by Cluster')\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Survival Rate (%)')\n",
    "plt.xticks(cluster_df['cluster_id'])\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Integration with Transition Matrix Analysis\n",
    "\n",
    "ETS clustering is particularly useful as part of Archetypal Path Analysis, where we track data points through multiple layers of a neural network. Let's demonstrate how to use ETS with transition matrices to analyze paths through a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transition matrix utilities\n",
    "from concept_fragmentation.analysis.transition_matrix import (\n",
    "    compute_transition_matrix,\n",
    "    compute_transition_entropy\n",
    ")\n",
    "\n",
    "# Generate multi-layer synthetic data\n",
    "def generate_layered_data(n_samples=200, n_layers=3, n_features_per_layer=None, n_clusters_per_layer=None):\n",
    "    \"\"\"Generate synthetic data representing multiple layers of a neural network.\"\"\"\n",
    "    # Set defaults\n",
    "    if n_features_per_layer is None:\n",
    "        n_features_per_layer = [5, 8, 6]\n",
    "    if n_clusters_per_layer is None:\n",
    "        n_clusters_per_layer = [3, 4, 3]\n",
    "    \n",
    "    # Ensure we have the right number of layers\n",
    "    n_layers = len(n_features_per_layer)\n",
    "    assert len(n_clusters_per_layer) == n_layers, \"Must provide cluster counts for all layers\"\n",
    "    \n",
    "    # Generate data for each layer\n",
    "    layers = {}\n",
    "    true_clusters = {}\n",
    "    \n",
    "    for i in range(n_layers):\n",
    "        layer_name = f\"layer{i+1}\"\n",
    "        X, y = generate_synthetic_data(\n",
    "            n_samples=n_samples,\n",
    "            n_clusters=n_clusters_per_layer[i],\n",
    "            n_features=n_features_per_layer[i],\n",
    "            cluster_std=0.3\n",
    "        )\n",
    "        layers[layer_name] = X\n",
    "        true_clusters[layer_name] = y\n",
    "    \n",
    "    return layers, true_clusters\n",
    "\n",
    "# Generate layered data\n",
    "layers, true_clusters = generate_layered_data(\n",
    "    n_samples=200,\n",
    "    n_features_per_layer=[4, 6, 5],\n",
    "    n_clusters_per_layer=[3, 4, 3]\n",
    ")\n",
    "\n",
    "# Apply ETS clustering to each layer\n",
    "ets_clusters = {}\n",
    "threshold_percentile = 0.1\n",
    "\n",
    "for layer_name, layer_data in layers.items():\n",
    "    print(f\"Clustering {layer_name}...\")\n",
    "    labels, thresholds = compute_ets_clustering(\n",
    "        layer_data,\n",
    "        threshold_percentile=threshold_percentile\n",
    "    )\n",
    "    ets_clusters[layer_name] = labels\n",
    "    print(f\"  Found {len(np.unique(labels))} clusters\")\n",
    "\n",
    "# Compute transition matrices between consecutive layers\n",
    "transitions = {}\n",
    "layer_names = sorted(layers.keys())\n",
    "\n",
    "for i in range(len(layer_names)-1):\n",
    "    source = layer_names[i]\n",
    "    target = layer_names[i+1]\n",
    "    transition_name = f\"{source}_to_{target}\"\n",
    "    \n",
    "    # Compute transition matrix\n",
    "    transition_matrix = compute_transition_matrix(\n",
    "        ets_clusters[source],\n",
    "        ets_clusters[target]\n",
    "    )\n",
    "    \n",
    "    transitions[transition_name] = transition_matrix\n",
    "    \n",
    "    # Compute transition entropy metrics\n",
    "    entropy = compute_transition_entropy(transition_matrix)\n",
    "    \n",
    "    print(f\"\\nTransition: {transition_name}\")\n",
    "    print(f\"Mean entropy: {entropy['mean_entropy']:.3f}\")\n",
    "    print(f\"Normalized entropy: {entropy['normalized_entropy']:.3f}\")\n",
    "    print(f\"Sparsity: {entropy['sparsity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transition matrices\n",
    "for name, matrix in transitions.items():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(matrix, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(label='Transition Probability')\n",
    "    plt.title(f'Transition Matrix: {name}')\n",
    "    plt.xlabel('Target Cluster')\n",
    "    plt.ylabel('Source Cluster')\n",
    "    \n",
    "    # Add text labels for significant transitions\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            if matrix[i, j] > 0.2:  # Only label significant transitions\n",
    "                plt.text(j, i, f'{matrix[i, j]:.2f}', \n",
    "                        ha='center', va='center', \n",
    "                        color='white' if matrix[i, j] > 0.5 else 'black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Extract and analyze paths\n",
    "def extract_paths(layer_clusters):\n",
    "    \"\"\"Extract paths from layer clusters.\"\"\"\n",
    "    # Get ordered layer names\n",
    "    layer_names = sorted(layer_clusters.keys())\n",
    "    \n",
    "    # Get number of samples\n",
    "    n_samples = len(layer_clusters[layer_names[0]])\n",
    "    \n",
    "    # Create paths array\n",
    "    n_layers = len(layer_names)\n",
    "    paths = np.zeros((n_samples, n_layers), dtype=int)\n",
    "    \n",
    "    # Fill in paths\n",
    "    for i, layer in enumerate(layer_names):\n",
    "        paths[:, i] = layer_clusters[layer]\n",
    "    \n",
    "    return paths\n",
    "\n",
    "# Extract paths\n",
    "paths = extract_paths(ets_clusters)\n",
    "\n",
    "# Count and display the most common paths\n",
    "unique_paths, path_counts = np.unique(paths, axis=0, return_counts=True)\n",
    "path_idx = np.argsort(-path_counts)\n",
    "\n",
    "print(f\"Found {len(unique_paths)} unique paths\")\n",
    "print(\"\\nMost common paths:\")\n",
    "for i in range(min(5, len(unique_paths))):\n",
    "    idx = path_idx[i]\n",
    "    path = unique_paths[idx]\n",
    "    count = path_counts[idx]\n",
    "    percentage = count / len(paths) * 100\n",
    "    print(f\"Path {path}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize path distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(min(10, len(unique_paths))), path_counts[path_idx[:10]] / len(paths) * 100)\n",
    "plt.title('Top 10 Path Distribution')\n",
    "plt.xlabel('Path Rank')\n",
    "plt.ylabel('Percentage of Samples')\n",
    "plt.grid(True, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Tips\n",
    "\n",
    "Here are some recommendations for using ETS effectively in your projects:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Selecting the Right Threshold Percentile\n",
    "\n",
    "The threshold percentile is the most important parameter in ETS clustering. Here are some guidelines:\n",
    "\n",
    "- **Low percentiles (0.01-0.05)**: Create more fine-grained clusters, good for detailed analysis\n",
    "- **Medium percentiles (0.1-0.2)**: Balanced approach, works well for most datasets\n",
    "- **High percentiles (0.3-0.5)**: Create broader clusters, good for high-level patterns\n",
    "\n",
    "To find the optimal threshold, you can:\n",
    "1. Start with a default of 0.1\n",
    "2. Try a range of values and evaluate using silhouette score or ARI (if ground truth is available)\n",
    "3. Visualize the clusters to ensure they make intuitive sense\n",
    "\n",
    "### 7.2 Handling Large Datasets\n",
    "\n",
    "ETS has built-in optimizations for large datasets:\n",
    "\n",
    "- **Batch processing**: Use the `batch_size` parameter to control memory usage\n",
    "- **Non-vectorized fallback**: Automatically used for very large similarity matrices\n",
    "- **Sampling**: For extremely large datasets, consider using a representative sample first\n",
    "\n",
    "### 7.3 Interpreting ETS Results\n",
    "\n",
    "When analyzing ETS clustering results, consider:\n",
    "\n",
    "1. **Dimension importance**: Which dimensions are most active in defining clusters?\n",
    "2. **Cluster characteristics**: What are the typical values for each feature in each cluster?\n",
    "3. **Pairwise explanations**: Use `explain_ets_similarity` to understand specific cluster assignments\n",
    "4. **Cluster statistics**: Use `compute_ets_statistics` to get insights about cluster properties\n",
    "\n",
    "### 7.4 Integration with Other Methods\n",
    "\n",
    "ETS works well with other analysis techniques:\n",
    "\n",
    "- **Dimensionality reduction**: Use PCA or UMAP for visualization after clustering\n",
    "- **Transition analysis**: Combine with transition matrices for path analysis\n",
    "- **Traditional clustering**: Compare results with k-means or DBSCAN for validation\n",
    "- **Feature selection**: Use dimension importance to identify key features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we've explored Explainable Threshold Similarity (ETS) clustering, a method that provides transparent, dimension-wise cluster definitions. Key takeaways:\n",
    "\n",
    "1. ETS automatically adapts to different feature scales without preprocessing\n",
    "2. The threshold percentile parameter controls the granularity of clustering\n",
    "3. ETS provides detailed explanations of why points belong to the same or different clusters\n",
    "4. It integrates well with transition matrix analysis for multi-layer path tracking\n",
    "5. The method provides statistical insights about cluster characteristics and feature importance\n",
    "\n",
    "ETS is particularly valuable for interpretability tasks, where understanding why models cluster data in a certain way is as important as the clustering itself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}